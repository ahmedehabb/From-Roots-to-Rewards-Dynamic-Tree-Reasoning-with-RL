{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritize notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import question_answering as question_answering\n",
    "question_answering = reload(question_answering)\n",
    "from question_answering import aggregate_multihop_answer as aggregate_multihop_answer_hotpotqa, get_cb_answer as get_cb_answer_hotpotqa, get_singlehop_ob_answer as get_singlehop_ob_answer_hotpotqa, get_multihop_ob_answer as get_multihop_ob_answer_hotpotqa\n",
    "\n",
    "import question_answering_2wiki as question_answering_2wiki\n",
    "question_answering_2wiki = reload(question_answering_2wiki)\n",
    "from question_answering_2wiki import aggregate_multihop_answer as aggregate_multihop_answer_2wiki, get_cb_answer as get_cb_answer_2wiki, get_singlehop_ob_answer as get_singlehop_ob_answer_2wiki, get_multihop_ob_answer as get_multihop_ob_answer_2wiki\n",
    "\n",
    "import question_answering_musique as question_answering_musique\n",
    "question_answering_musique = reload(question_answering_musique)\n",
    "from question_answering_musique import aggregate_multihop_answer as aggregate_multihop_answer_musique, get_cb_answer as get_cb_answer_musique, get_singlehop_ob_answer as get_singlehop_ob_answer_musique, get_multihop_ob_answer as get_multihop_ob_answer_musique\n",
    "\n",
    "from question_answering import togetherai_caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_answer(question, dataset):\n",
    "    \"\"\"\n",
    "    Get the answer from the dataset.\n",
    "    \"\"\"\n",
    "    if dataset == \"hotpotqa\":\n",
    "        return get_cb_answer_hotpotqa(question)\n",
    "    elif dataset == \"2wiki\":\n",
    "        return get_cb_answer_2wiki(question)\n",
    "    elif dataset == \"musique\":\n",
    "        return get_cb_answer_musique(question)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name. Choose from 'hotpotqa', '2wiki', or 'musique'.\")\n",
    "    \n",
    "def get_singlehop_ob_answer(question, topic_entities, dataset):\n",
    "    \"\"\"\n",
    "    Get the single-hop open-book answer from the dataset.\n",
    "    \"\"\"\n",
    "    if dataset == \"hotpotqa\":\n",
    "        return get_singlehop_ob_answer_hotpotqa(question, topic_entities)\n",
    "    elif dataset == \"2wiki\":\n",
    "        return get_singlehop_ob_answer_2wiki(question, topic_entities)\n",
    "    elif dataset == \"musique\":\n",
    "        return get_singlehop_ob_answer_musique(question, topic_entities)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name. Choose from 'hotpotqa', '2wiki', or 'musique'.\")\n",
    "    \n",
    "def get_multihop_ob_answer(question, tree, dataset):\n",
    "    \"\"\"\n",
    "    Get the multi-hop open-book answer from the dataset.\n",
    "    \"\"\"\n",
    "    if dataset == \"hotpotqa\":\n",
    "        return get_multihop_ob_answer_hotpotqa(question, tree)\n",
    "    elif dataset == \"2wiki\":\n",
    "        return get_multihop_ob_answer_2wiki(question, tree)\n",
    "    elif dataset == \"musique\":\n",
    "        return get_multihop_ob_answer_musique(question, tree)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name. Choose from 'hotpotqa', '2wiki', or 'musique'.\")\n",
    "    \n",
    "def aggregate_multihop_answer(question, tree, dataset):\n",
    "    \"\"\"\n",
    "    Aggregate the multi-hop answer from the dataset.\n",
    "    \"\"\"\n",
    "    if dataset == \"hotpotqa\":\n",
    "        return aggregate_multihop_answer_hotpotqa(question, tree)\n",
    "    elif dataset == \"2wiki\":\n",
    "        return aggregate_multihop_answer_2wiki(question, tree)\n",
    "    elif dataset == \"musique\":\n",
    "        return aggregate_multihop_answer_musique(question, tree)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name. Choose from 'hotpotqa', '2wiki', or 'musique'.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_answers_equivalent_using_llm(gold, candidate):\n",
    "    \"\"\"\n",
    "    Compare two answers to determine their semantic equivalence using an LLM.\n",
    "    Returns True if answers are equivalent; False otherwise.\n",
    "\n",
    "    Args:\n",
    "        gold (str): The gold answer.\n",
    "        candidate (str): The candidate answer.\n",
    "    \"\"\"\n",
    "    # Few-shot prompt\n",
    "    prompt = f\"\"\"\n",
    "    I want you to compare two answers and determine if they are equivalent.\n",
    "    You should respond with \"Yes\" if the two answers have the same meaning, even if they are written differently. \n",
    "    Respond with \"No\" if they do not have the same meaning.\n",
    "\n",
    "    Here are some examples:\n",
    "\n",
    "    Gold Answer: \"two\"\n",
    "    Candidate Answer: \"2\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"Paris\"\n",
    "    Candidate Answer: \"the capital city of France\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"four hundred years\"\n",
    "    Candidate Answer: \"400 years\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"climate change\"\n",
    "    Candidate Answer: \"global warming\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"The Eiffel Tower\"\n",
    "    Candidate Answer: \"a landmark in Paris\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"3 meters\"\n",
    "    Candidate Answer: \"300 centimeters\"\n",
    "    Are these answers equivalent? Yes\n",
    "\n",
    "    Gold Answer: \"apple\"\n",
    "    Candidate Answer: \"orange\"\n",
    "    Are these answers equivalent? No\n",
    "\n",
    "    Gold Answer: \"2023\"\n",
    "    Candidate Answer: \"2022\"\n",
    "    Are these answers equivalent? No\n",
    "\n",
    "    Gold Answer: \"Canada\"\n",
    "    Candidate Answer: \"United States\"\n",
    "    Are these answers equivalent? No\n",
    "\n",
    "    Now, here is the pair for you to evaluate:\n",
    "\n",
    "    Gold Answer: \"{gold}\"\n",
    "    Candidate Answer: \"{candidate}\"\n",
    "    Are these answers equivalent?\n",
    "    \"\"\"\n",
    "\n",
    "    # Make API call\n",
    "    try:\n",
    "        # print(\"Prompt:\", prompt)\n",
    "        # Use LLM API to get the reformulated question\n",
    "        response, tag = togetherai_caller.req2provider(prompt=prompt, max_tokens=None, stop= None, use_cache=True)\n",
    "        response = response[0]\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        \n",
    "        if \"yes\" in answer:\n",
    "            return True\n",
    "        elif \"no\" in answer:\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected LLM response: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM API call: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_answers_equivalent_using_llm(\"Two\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import copy\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        print(f\"Error processing item: {item}, Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n",
    "\n",
    "# Initialize counters\n",
    "correct_answers = 0\n",
    "total_parent_nodes = 0\n",
    "# Initialize lists to store logprobs\n",
    "correct_logprobs = []\n",
    "incorrect_logprobs = []\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []  # Features (logprobs)\n",
    "y = []  # Labels (1 for correct, 0 for incorrect)\n",
    "\n",
    "# Loop through each example in the data\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        # Check if the node is a parent node (no \"fa\" entry)\n",
    "        if \"fa\" not in node:\n",
    "            total_parent_nodes += 1\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            answer = node.get(\"answer\", [None])[0]  # Extract the best answer\n",
    "            # Get the gold answer from q2gold\n",
    "            if question_text in q2gold:\n",
    "                gold_answer, _ = q2gold[question_text]\n",
    "                print(f\"Question: {question_text}, Gold: {gold_answer}, answer: {answer}\")\n",
    "                # Compare cb_answer_text with the gold answer\n",
    "                if are_answers_equivalent_using_llm(gold_answer, answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times probtree answer matches gold answer: {correct_answers}\")\n",
    "print(f\"correct answers match rate for probtree answers: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed book answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n",
    "\n",
    "# Initialize counters\n",
    "correct_cb_answers = 0\n",
    "total_parent_nodes = 0\n",
    "# Initialize lists to store logprobs\n",
    "correct_logprobs = []\n",
    "incorrect_logprobs = []\n",
    "# Initialize lists to store lengths\n",
    "correct_lengths = []\n",
    "incorrect_lengths = []\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []  # Features (logprobs)\n",
    "y = []  # Labels (1 for correct, 0 for incorrect)\n",
    "\n",
    "# Loop through each example in the data\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        # Check if the node is a parent node (no \"fa\" entry)\n",
    "        if \"fa\" not in node:\n",
    "            total_parent_nodes += 1\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            cb_answer = node.get(\"cb_answer\", [None])  # Extract the cb_answer\n",
    "            cb_answer_text = cb_answer[0]\n",
    "            cb_logprob = cb_answer[1]  # Extract logprobs\n",
    "            cb_logprobs = cb_answer[3]  # Extract logprobs, now i am taking the whole sequence\n",
    "\n",
    "            # Get the gold answer from q2gold\n",
    "            if question_text in q2gold:\n",
    "                gold_answer, _ = q2gold[question_text]\n",
    "\n",
    "                # Compare cb_answer_text with the gold answer\n",
    "                # if normalize_answer(cb_answer_text) == normalize_answer(gold_answer):\n",
    "                if are_answers_equivalent_using_llm(gold_answer, cb_answer_text):\n",
    "                    print(f\"CB Answer: {cb_answer_text}, Gold Answer: {gold_answer} : Correct\")\n",
    "                    if cb_logprob > -10:\n",
    "                        correct_cb_answers += 1\n",
    "                        correct_logprobs.append(cb_logprob)  # Save logprobs for correct cases\n",
    "                        correct_lengths.append(len(cb_logprobs))  # Save lengths for correct cases\n",
    "                        # X.append([cb_logprob])  # Feature\n",
    "                        X.append(cb_logprobs)  # Feature\n",
    "                        y.append(1)  # Label (correct)\n",
    "                else:\n",
    "                    print(f\"CB Answer: {cb_answer_text}, Gold Answer: {gold_answer} : Incorrect\")\n",
    "                    if cb_logprob > -10:\n",
    "                        incorrect_logprobs.append(cb_logprob)  # Save logprobs for incorrect cases\n",
    "                        incorrect_lengths.append(len(cb_logprobs))\n",
    "                        # X.append([cb_logprob])  # Feature\n",
    "                        X.append(cb_logprobs)  # Feature\n",
    "                        y.append(0)  # Label (incorrect)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_cb_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times cb_answer matches gold answer: {correct_cb_answers}\")\n",
    "print(f\"Closed-book match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Pad/truncate sequences to the maximum log prob sequence length\n",
    "max_length = max(len(seq) for seq in X)\n",
    "X_cb_padded = pad_sequences(X, maxlen=max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate statistics for correct cases\n",
    "correct_mean = np.mean(correct_logprobs) if correct_logprobs else 0\n",
    "correct_std = np.std(correct_logprobs) if correct_logprobs else 0\n",
    "\n",
    "# Calculate statistics for incorrect cases\n",
    "incorrect_mean = np.mean(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "incorrect_std = np.std(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Correct Cases:\")\n",
    "print(f\"  Number of cases: {len(correct_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {correct_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {correct_std:.4f}\")\n",
    "\n",
    "print(\"\\nIncorrect Cases:\")\n",
    "print(f\"  Number of cases: {len(incorrect_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {incorrect_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {incorrect_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Calculate statistics for correct cases\n",
    "correct_mean = np.mean(correct_logprobs) if correct_logprobs else 0\n",
    "correct_std = np.std(correct_logprobs) if correct_logprobs else 0\n",
    "\n",
    "# Calculate statistics for incorrect cases\n",
    "incorrect_mean = np.mean(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "incorrect_std = np.std(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Correct Cases:\")\n",
    "print(f\"  Number of cases: {len(correct_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {correct_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {correct_std:.4f}\")\n",
    "\n",
    "print(\"\\nIncorrect Cases:\")\n",
    "print(f\"  Number of cases: {len(incorrect_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {incorrect_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {incorrect_std:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_logprobs, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_logprobs, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_logprobs, incorrect_logprobs], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "cb_logprob_correct_mean = np.mean(correct_logprobs)\n",
    "cb_logprob_correct_std = np.std(correct_logprobs)\n",
    "cb_logprob_incorrect_mean = np.mean(incorrect_logprobs)\n",
    "cb_logprob_incorrect_std = np.std(incorrect_logprobs)\n",
    "\n",
    "print(\"Correct - Mean:\", cb_logprob_correct_mean, \"Std:\", cb_logprob_correct_std)\n",
    "print(\"Incorrect - Mean:\", cb_logprob_incorrect_mean, \"Std:\", cb_logprob_incorrect_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(correct_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(incorrect_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_lengths, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_lengths, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_lengths, incorrect_lengths], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "cb_length_correct_mean = np.mean(correct_lengths)\n",
    "cb_length_correct_std = np.std(correct_lengths)\n",
    "cb_length_incorrect_mean = np.mean(incorrect_lengths)\n",
    "cb_length_incorrect_std = np.std(incorrect_lengths)\n",
    "\n",
    "print(\"Correct - Mean:\", cb_length_correct_mean, \"Std:\", cb_length_correct_std)\n",
    "print(\"Incorrect - Mean:\", cb_length_incorrect_mean, \"Std:\", cb_length_incorrect_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cb_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class Distribution:\", np.bincount(y))\n",
    "\n",
    "# Train a Logistic Regression model with class weights\n",
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n",
    "\n",
    "# Adjust threshold\n",
    "for i in range(0, 10):\n",
    "    threshold = i/10  # Experiment with different thresholds\n",
    "    # threshold = 0.2  # Experiment with different thresholds\n",
    "    y_pred_adj = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_adj)\n",
    "    precision = precision_score(y_test, y_pred_adj)\n",
    "    recall = recall_score(y_test, y_pred_adj)\n",
    "    f1 = f1_score(y_test, y_pred_adj)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"Model Evaluation (Adjusted Threshold): {threshold}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n",
    "\n",
    "# Initialize counters\n",
    "correct_child_answers = 0\n",
    "total_parent_nodes = 0\n",
    "# Initialize lists to store logprobs\n",
    "correct_logprobs = []\n",
    "incorrect_logprobs = []\n",
    "# Initialize lists to store lengths\n",
    "correct_lengths = []\n",
    "incorrect_lengths = []\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []  # Features (logprobs)\n",
    "y = []  # Labels (1 for correct, 0 for incorrect)\n",
    "\n",
    "# Loop through each example in the data\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        # Check if the node is a parent node (no \"fa\" entry)\n",
    "        if \"fa\" not in node:\n",
    "            total_parent_nodes += 1\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            child_answer = node.get(\"child_answer\", [None])  # Extract the child_answer\n",
    "            child_answer_text = child_answer[0]\n",
    "            child_logprob = child_answer[1]  # Extract logprobs\n",
    "            child_logprobs = child_answer[3]  # Extract logprobs\n",
    "\n",
    "            # Get the gold answer from q2gold\n",
    "            if question_text in q2gold:\n",
    "                gold_answer, _ = q2gold[question_text]\n",
    "\n",
    "                # Compare child_answer_text with the gold answer\n",
    "                # if normalize_answer(child_answer_text) == normalize_answer(gold_answer):\n",
    "                if are_answers_equivalent_using_llm(gold_answer, child_answer_text):\n",
    "                    print(f\"OB Answer: {child_answer_text}, Gold Answer: {gold_answer} : Correct\")\n",
    "                    correct_child_answers += 1\n",
    "                    correct_logprobs.append(child_logprob)  # Save logprobs for correct cases\n",
    "                    correct_lengths.append(len(child_logprobs))\n",
    "                    # X.append([child_logprob])  # Feature\n",
    "                    X.append(child_logprobs)  # Feature\n",
    "                    y.append(1)  # Label (correct)\n",
    "                else:\n",
    "                    print(f\"OB Answer: {child_answer_text}, Gold Answer: {gold_answer} : Incorrect\")\n",
    "                    if child_logprob > -10:\n",
    "                        incorrect_logprobs.append(child_logprob)  # Save logprobs for incorrect cases\n",
    "                        incorrect_lengths.append(len(child_logprobs))\n",
    "                        # X.append([child_logprob])  # Feature\n",
    "                        X.append(child_logprobs)  # Feature\n",
    "                        y.append(0)  # Label (incorrect)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_child_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times child_answer matches gold answer: {correct_child_answers}\")\n",
    "print(f\"Open-book match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Calculate statistics for correct cases\n",
    "correct_mean = np.mean(correct_logprobs) if correct_logprobs else 0\n",
    "correct_std = np.std(correct_logprobs) if correct_logprobs else 0\n",
    "\n",
    "# Calculate statistics for incorrect cases\n",
    "incorrect_mean = np.mean(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "incorrect_std = np.std(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Correct Cases:\")\n",
    "print(f\"  Number of cases: {len(correct_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {correct_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {correct_std:.4f}\")\n",
    "\n",
    "print(\"\\nIncorrect Cases:\")\n",
    "print(f\"  Number of cases: {len(incorrect_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {incorrect_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {incorrect_std:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_logprobs, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_logprobs, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_logprobs, incorrect_logprobs], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "child_logprob_correct_mean = np.mean(correct_logprobs)\n",
    "child_logprob_correct_std = np.std(correct_logprobs)\n",
    "child_logprob_incorrect_mean = np.mean(incorrect_logprobs)\n",
    "child_logprob_incorrect_std = np.std(incorrect_logprobs)\n",
    "\n",
    "print(\"Correct - Mean:\", child_logprob_correct_mean, \"Std:\", child_logprob_correct_std)\n",
    "print(\"Incorrect - Mean:\", child_logprob_incorrect_mean, \"Std:\", child_logprob_incorrect_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(correct_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(incorrect_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_lengths, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_lengths, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_lengths, incorrect_lengths], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "child_length_correct_mean = np.mean(correct_lengths)\n",
    "child_length_correct_std = np.std(correct_lengths)\n",
    "child_length_incorrect_mean = np.mean(incorrect_lengths)\n",
    "child_length_incorrect_std = np.std(incorrect_lengths)\n",
    "\n",
    "print(\"Correct - Mean:\", child_length_correct_mean, \"Std:\", child_length_correct_std)\n",
    "print(\"Incorrect - Mean:\", child_length_incorrect_mean, \"Std:\", child_length_incorrect_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openbook answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n",
    "\n",
    "# Initialize counters\n",
    "correct_ob_answers = 0\n",
    "total_parent_nodes = 0\n",
    "# Initialize lists to store logprobs\n",
    "correct_logprobs = []\n",
    "incorrect_logprobs = []\n",
    "# Initialize lists to store lengths\n",
    "correct_lengths = []\n",
    "incorrect_lengths = []\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []  # Features (logprobs)\n",
    "y = []  # Labels (1 for correct, 0 for incorrect)\n",
    "\n",
    "# Loop through each example in the data\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        # Check if the node is a parent node (no \"fa\" entry)\n",
    "        if \"fa\" not in node:\n",
    "            total_parent_nodes += 1\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            ob_answer = node.get(\"ob_answer\", [None])  # Extract the ob_answer\n",
    "            ob_answer_text = ob_answer[0]\n",
    "            ob_logprob = ob_answer[1]  # Extract logprobs\n",
    "            ob_logprobs = ob_answer[3]  # Extract logprobs\n",
    "\n",
    "            # Get the gold answer from q2gold\n",
    "            if question_text in q2gold:\n",
    "                gold_answer, _ = q2gold[question_text]\n",
    "\n",
    "                # Compare ob_answer_text with the gold answer\n",
    "                # if normalize_answer(ob_answer_text) == normalize_answer(gold_answer):\n",
    "                if are_answers_equivalent_using_llm(gold_answer, ob_answer_text):\n",
    "                    print(f\"OB Answer: {ob_answer_text}, Gold Answer: {gold_answer} : Correct\")\n",
    "                    correct_ob_answers += 1\n",
    "                    correct_logprobs.append(ob_logprob)  # Save logprobs for correct cases\n",
    "                    correct_lengths.append(len(ob_logprobs))\n",
    "                    # X.append([ob_logprob])  # Feature\n",
    "                    X.append(ob_logprobs)  # Feature\n",
    "                    y.append(1)  # Label (correct)\n",
    "                else:\n",
    "                    print(f\"OB Answer: {ob_answer_text}, Gold Answer: {gold_answer} : Incorrect\")\n",
    "                    if ob_logprob > -10:\n",
    "                        incorrect_logprobs.append(ob_logprob)  # Save logprobs for incorrect cases\n",
    "                        incorrect_lengths.append(len(ob_logprobs))\n",
    "                        # X.append([ob_logprob])  # Feature\n",
    "                        X.append(ob_logprobs)  # Feature\n",
    "                        y.append(0)  # Label (incorrect)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_ob_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times ob_answer matches gold answer: {correct_ob_answers}\")\n",
    "print(f\"Open-book match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate statistics for correct cases\n",
    "correct_mean = np.mean(correct_logprobs) if correct_logprobs else 0\n",
    "correct_std = np.std(correct_logprobs) if correct_logprobs else 0\n",
    "\n",
    "# Calculate statistics for incorrect cases\n",
    "incorrect_mean = np.mean(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "incorrect_std = np.std(incorrect_logprobs) if incorrect_logprobs else 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Correct Cases:\")\n",
    "print(f\"  Number of cases: {len(correct_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {correct_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {correct_std:.4f}\")\n",
    "\n",
    "print(\"\\nIncorrect Cases:\")\n",
    "print(f\"  Number of cases: {len(incorrect_logprobs)}\")\n",
    "print(f\"  Mean logprobs: {incorrect_mean:.4f}\")\n",
    "print(f\"  Standard deviation of logprobs: {incorrect_std:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_logprobs, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_logprobs, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_logprobs, incorrect_logprobs], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "ob_logprob_correct_mean = np.mean(correct_logprobs)\n",
    "ob_logprob_correct_std = np.std(correct_logprobs)\n",
    "ob_logprob_incorrect_mean = np.mean(incorrect_logprobs)\n",
    "ob_logprob_incorrect_std = np.std(incorrect_logprobs)\n",
    "\n",
    "print(\"Correct - Mean:\", ob_logprob_correct_mean, \"Std:\", ob_logprob_correct_std)\n",
    "print(\"Incorrect - Mean:\", ob_logprob_incorrect_mean, \"Std:\", ob_logprob_incorrect_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(correct_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of log_probs\n",
    "plt.hist(incorrect_logprobs, bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of log_probs\")\n",
    "plt.xlabel(\"log_prob\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(correct_lengths, color=\"green\", label=\"Correct\", kde=True, bins=20)\n",
    "sns.histplot(incorrect_lengths, color=\"red\", label=\"Incorrect\", kde=True, bins=20)\n",
    "plt.title(\"Distribution of Logprobs\")\n",
    "plt.xlabel(\"Logprobs\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot of logprobs for correct and incorrect cases\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=[correct_lengths, incorrect_lengths], palette=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], [\"Correct\", \"Incorrect\"])\n",
    "plt.title(\"Boxplot of Logprobs\")\n",
    "plt.xlabel(\"Case Type\")\n",
    "plt.ylabel(\"Logprobs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "ob_length_correct_mean = np.mean(correct_lengths)\n",
    "ob_length_correct_std = np.std(correct_lengths)\n",
    "ob_length_incorrect_mean = np.mean(incorrect_lengths)\n",
    "ob_length_incorrect_std = np.std(incorrect_lengths)\n",
    "\n",
    "print(\"Correct - Mean:\", ob_length_correct_mean, \"Std:\", ob_length_correct_std)\n",
    "print(\"Incorrect - Mean:\", ob_length_incorrect_mean, \"Std:\", ob_length_incorrect_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad/truncate sequences to the maximum log prob sequence length\n",
    "max_length = max(len(seq) for seq in X)\n",
    "X_ob_padded = pad_sequences(X, maxlen=max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "X_ob_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ob_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class Distribution:\", np.bincount(y))\n",
    "\n",
    "# Train a Logistic Regression model with class weights\n",
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n",
    "\n",
    "# loop on thresholds from 0.1 to 0.9\n",
    "for i in range(1, 10):\n",
    "    threshold = i / 10\n",
    "\n",
    "    # Adjust threshold\n",
    "    # threshold = i  # Experiment with different thresholds\n",
    "    y_pred_adj = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_adj)\n",
    "    precision = precision_score(y_test, y_pred_adj)\n",
    "    recall = recall_score(y_test, y_pred_adj)\n",
    "    f1 = f1_score(y_test, y_pred_adj)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(\"Model Evaluation (Adjusted Threshold):\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Random forest classifier for each type of answer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n",
    "\n",
    "# Initialize counters\n",
    "correct_answers = 0\n",
    "total_parent_nodes = 0\n",
    "# Initialize lists to store logprobs\n",
    "correct_logprobs = []\n",
    "incorrect_logprobs = []\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []  # Features (logprobs)\n",
    "y = []  # Labels (1 for correct, 0 for incorrect)\n",
    "\n",
    "# Loop through each example in the data\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        # Check if the node is a parent node (no \"fa\" entry)\n",
    "        if \"fa\" not in node:\n",
    "            total_parent_nodes += 1\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            ob_answer = node.get(\"ob_answer\", [None])  # Extract the ob_answer\n",
    "            ob_answer_text = ob_answer[0]\n",
    "            cb_answer_text = node.get(\"cb_answer\", [None])[0]\n",
    "\n",
    "\n",
    "            # Get the gold answer from q2gold\n",
    "            if question_text in q2gold:\n",
    "                gold_answer, _ = q2gold[question_text]\n",
    "\n",
    "                # Compare ob_answer_text with the gold answer\n",
    "                # if normalize_answer(ob_answer_text) == normalize_answer(gold_answer) or normalize_answer(cb_answer_text) == normalize_answer(gold_answer):\n",
    "                if are_answers_equivalent_using_llm(gold_answer, ob_answer_text) or are_answers_equivalent_using_llm(gold_answer, cb_answer_text):\n",
    "                    print(f\"OB Answer: {ob_answer_text}, CB Answer: {cb_answer_text}, Gold Answer: {gold_answer} : Correct\")\n",
    "                    correct_answers += 1\n",
    "                else:\n",
    "                    print(f\"OB Answer: {ob_answer_text}, CB Answer: {cb_answer_text}, Gold Answer: {gold_answer} : Incorrect\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times cb_answer & ob_answer matches gold answer: {correct_answers}\")\n",
    "print(f\"Closed-book & Open-book match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract features and labels for parent nodes\n",
    "X_cb = []  # CB logprobs\n",
    "X_ob = []  # OB logprobs\n",
    "X_child = []  # Child logprobs\n",
    "y_cb = []  # 1 if CB answer matches final answer, else 0\n",
    "y_ob = []  # 1 if OB answer matches final answer, else 0\n",
    "y_child = []  # 1 if Child answer matches final answer, else 0\n",
    "\n",
    "for example in data:\n",
    "    for node in example:\n",
    "        if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "            cb_answer = node.get(\"cb_answer\", [None, None])[0]\n",
    "            ob_answer = node.get(\"ob_answer\", [None, None])[0]\n",
    "            child_answer = node.get(\"child_answer\", [None, None])[0]\n",
    "            cb_logprob = node.get(\"cb_answer\", [None, None])[3]\n",
    "            ob_logprob = node.get(\"ob_answer\", [None, None])[3]\n",
    "            child_logprob = node.get(\"child_answer\", [None, None])[3]\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            final_answer, _ = q2gold[question_text]\n",
    "\n",
    "            # Check if CB answer matches final answer\n",
    "            if are_answers_equivalent_using_llm(final_answer, cb_answer):\n",
    "                y_cb.append(1)\n",
    "            else:\n",
    "                y_cb.append(0)\n",
    "            X_cb.append(cb_logprob)\n",
    "\n",
    "            # Check if OB answer matches final answer\n",
    "            if are_answers_equivalent_using_llm(final_answer, ob_answer):\n",
    "                y_ob.append(1)\n",
    "            else:\n",
    "                y_ob.append(0)\n",
    "            X_ob.append(ob_logprob)\n",
    "\n",
    "            # Check if Child answer matches final answer\n",
    "            if child_answer and are_answers_equivalent_using_llm(final_answer, child_answer):\n",
    "                y_child.append(1)\n",
    "            else:\n",
    "                y_child.append(0)\n",
    "            X_child.append(child_logprob)\n",
    "\n",
    "# Pad/truncate sequences to the maximum log prob sequence length\n",
    "cb_max_length = max(len(seq) for seq in X_cb)\n",
    "X_cb_padded = pad_sequences(X_cb, maxlen=cb_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "ob_max_length = max(len(seq) for seq in X_ob)\n",
    "X_ob_padded = pad_sequences(X_ob, maxlen=ob_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "child_max_length = max(len(seq) for seq in X_child)\n",
    "X_child_padded = pad_sequences(X_child, maxlen=child_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_cb = np.array(X_cb_padded)\n",
    "X_ob = np.array(X_ob_padded)\n",
    "X_child = np.array(X_child_padded)\n",
    "y_cb = np.array(y_cb)\n",
    "y_ob = np.array(y_ob)\n",
    "y_child = np.array(y_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split into train and test sets\n",
    "X_cb_train, X_cb_test, y_cb_train, y_cb_test = train_test_split(X_cb, y_cb, test_size=0.2, random_state=42)\n",
    "X_ob_train, X_ob_test, y_ob_train, y_ob_test = train_test_split(X_ob, y_ob, test_size=0.2, random_state=42)\n",
    "X_child_train, X_child_test, y_child_train, y_child_test = train_test_split(X_child, y_child, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train CB model\n",
    "# cb_model = LogisticRegression(class_weight=\"balanced\")\n",
    "cb_model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "cb_model.fit(X_cb_train, y_cb_train)\n",
    "\n",
    "# Evaluate CB model\n",
    "y_cb_pred = cb_model.predict(X_cb_test)\n",
    "print(\"CB Model Accuracy:\", accuracy_score(y_cb_test, y_cb_pred))\n",
    "\n",
    "# Train OB model\n",
    "# ob_model = LogisticRegression(class_weight=\"balanced\")\n",
    "ob_model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "ob_model.fit(X_ob_train, y_ob_train)\n",
    "\n",
    "# Evaluate OB model\n",
    "y_ob_pred = ob_model.predict(X_ob_test)\n",
    "print(\"OB Model Accuracy:\", accuracy_score(y_ob_test, y_ob_pred))\n",
    "\n",
    "# Train Child model\n",
    "# child_model = LogisticRegression(class_weight=\"balanced\")\n",
    "child_model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "child_model.fit(X_child_train, y_child_train)\n",
    "\n",
    "# Evaluate Child model\n",
    "y_child_pred = child_model.predict(X_child_test)\n",
    "print(\"Child Model Accuracy:\", accuracy_score(y_child_test, y_child_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the order of which the models performed based on their accuracy from the best to the lowest\n",
    "model_accuracies = {\n",
    "    \"CB\": accuracy_score(y_cb_test, y_cb_pred),\n",
    "    \"OB\": accuracy_score(y_ob_test, y_ob_pred),\n",
    "    \"Child\": accuracy_score(y_child_test, y_child_pred)\n",
    "}\n",
    "\n",
    "model_dict = {\n",
    "    \"CB\": cb_model,\n",
    "    \"OB\": ob_model,\n",
    "    \"Child\": child_model,\n",
    "}\n",
    "\n",
    "# Sort the models based on their accuracy in descending order and save the order\n",
    "sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_models = [sorted_model[0] for sorted_model in sorted_models]\n",
    "\n",
    "sorted_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_node_by_idx(idx):\n",
    "    # Function to find a node by its index in the tree\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if node.get(\"idx\") == idx:\n",
    "                return node\n",
    "    return None\n",
    "\n",
    "def aggregate_answers(answers):\n",
    "    # Example aggregation logic: majority voting\n",
    "    from collections import Counter\n",
    "    if not answers:\n",
    "        return None\n",
    "    counter = Counter(answers)\n",
    "    return counter.most_common(1)[0][0]\n",
    "\n",
    "def greedy_solver(node, cb_model, ob_model, child_model):\n",
    "\n",
    "    # Step 1: Check OB answer\n",
    "    ob_answer, ob_logprob, _, ob_logprobs = node.get(\"ob_answer\")\n",
    "    # ob_reliable = ob_model.predict([[ob_logprob]])[0] == 1  # Predict reliability\n",
    "    ob_logprobs_padded = pad_sequences([ob_logprobs], maxlen=ob_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "    ob_reliable = ob_model.predict(ob_logprobs_padded)[0] == 1  # Predict reliability\n",
    "    if ob_reliable:\n",
    "        if \"unknown\" not in ob_answer.lower().strip():\n",
    "            print(\"OB answer is reliable\", ob_answer)\n",
    "            return ob_answer, \"OB\"  # Accept OB answer\n",
    "        else:\n",
    "            ob_logprob = -float('inf')\n",
    "    \n",
    "    # Step 2: Expand the Tree (Child Aggregation)\n",
    "    if \"sons\" in node and node[\"sons\"]:  # Check if node has children\n",
    "        child_answer, child_logprob, _, child_logprobs = node.get(\"child_answer\")\n",
    "        # child_reliable = child_model.predict([[child_logprob]])[0] == 1\n",
    "        child_logprobs_padded = pad_sequences([child_logprobs], maxlen=child_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "        child_reliable = child_model.predict(child_logprobs_padded)[0] == 1  # Predict reliability\n",
    "        if child_reliable:\n",
    "            if \"unknown\" not in child_answer.lower().strip():\n",
    "                print(\"Child answer is reliable\", child_answer)\n",
    "                return child_answer, \"Child\"  # Accept Child answer\n",
    "            else:\n",
    "                child_logprob = -float('inf')\n",
    "\n",
    "    # Step 3: Check CB answer\n",
    "    cb_answer, cb_logprob, _, cb_logprobs = node.get(\"cb_answer\")\n",
    "    # cb_reliable = cb_model.predict([[cb_logprob]])[0] == 1  # Predict reliability\n",
    "    cb_logprobs_padded = pad_sequences([cb_logprobs], maxlen=cb_max_length, padding=\"post\", truncating=\"post\", value=-100, dtype=\"float32\")\n",
    "    cb_reliable = cb_model.predict(cb_logprobs_padded)[0] == 1  # Predict reliability\n",
    "    if cb_reliable:\n",
    "        if \"unknown\" not in cb_answer.lower().strip():\n",
    "            print(\"CB answer is reliable\", cb_answer)\n",
    "            return cb_answer, \"CB\"  # Accept CB answer\n",
    "        else:\n",
    "            # set it to -ve infinity not to be chosen\n",
    "            cb_logprob = -float('inf')\n",
    "\n",
    "    # If no method produces a reliable answer, return the best available\n",
    "    print(\"No reliable answer found, returning the best available answer\")\n",
    "    if cb_logprob > ob_logprob and cb_logprob > child_logprob:\n",
    "        print(\"Best Answer: CB, Best Method: CB\")\n",
    "        return cb_answer, \"CB\"\n",
    "    elif ob_logprob > cb_logprob and ob_logprob > child_logprob:\n",
    "        print(\"Best Answer: OB, Best Method: OB\")\n",
    "        return ob_answer, \"OB\"\n",
    "    else:\n",
    "        print(\"Best Answer: Child, Best Method: Child\")\n",
    "        return child_answer, \"Child\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load test data\n",
    "with open('results-testset-hotpotqa.json', 'r') as file:\n",
    "    test_data_hotpotqa = json.load(file)\n",
    "with open('results-testset-2wiki.json', 'r') as file:\n",
    "    test_data_2wiki = json.load(file)\n",
    "with open('results-testset-musique.json', 'r') as file:\n",
    "    test_data_musique = json.load(file)\n",
    "\n",
    "# Load ground truth answers\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_test_random_500.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_test_random_500.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_test_random_500.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the greedy solver on all parent nodes\n",
    "\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "        \n",
    "results = []\n",
    "correct_answers = 0\n",
    "total_parent_nodes = 0\n",
    "for example in test_data_hotpotqa:\n",
    "    for node in example:\n",
    "        if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "            total_parent_nodes += 1\n",
    "            answer, method = greedy_solver(node, cb_model, ob_model, child_model)\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            final_answer, _ = q2gold[question_text]\n",
    "            # if normalize_answer(answer) == normalize_answer(final_answer):\n",
    "            if are_answers_equivalent_using_llm(final_answer, answer):\n",
    "                correct_answers += 1\n",
    "            results.append({\n",
    "                \"idx\": node[\"idx\"],\n",
    "                \"question\": node[\"question_text\"],\n",
    "                \"answer\": answer,\n",
    "                \"method\": method\n",
    "            })\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Node {result['idx']}:\")\n",
    "    print(f\"  Question: {result['question']}\")\n",
    "    print(f\"  Answer: {result['answer']}\")\n",
    "    print(f\"  Method: {result['method']}\")\n",
    "    print()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times answer matches gold answer: {correct_answers}\")\n",
    "print(f\"Greedy Solver match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the greedy solver on all parent nodes\n",
    "\n",
    "q2gold = {}\n",
    "for item in raw_data_2wiki:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "        \n",
    "results = []\n",
    "correct_answers = 0\n",
    "total_parent_nodes = 0\n",
    "for example in test_data_2wiki:\n",
    "    for node in example:\n",
    "        if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "            total_parent_nodes += 1\n",
    "            answer, method = greedy_solver(node, cb_model, ob_model, child_model)\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            final_answer, _ = q2gold[question_text]\n",
    "            # if normalize_answer(answer) == normalize_answer(final_answer):\n",
    "            if are_answers_equivalent_using_llm(final_answer, answer):\n",
    "                correct_answers += 1\n",
    "            results.append({\n",
    "                \"idx\": node[\"idx\"],\n",
    "                \"question\": node[\"question_text\"],\n",
    "                \"answer\": answer,\n",
    "                \"method\": method\n",
    "            })\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Node {result['idx']}:\")\n",
    "    print(f\"  Question: {result['question']}\")\n",
    "    print(f\"  Answer: {result['answer']}\")\n",
    "    print(f\"  Method: {result['method']}\")\n",
    "    print()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times answer matches gold answer: {correct_answers}\")\n",
    "print(f\"Greedy Solver match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the greedy solver on all parent nodes\n",
    "\n",
    "q2gold = {}\n",
    "for item in raw_data_musique:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "        \n",
    "results = []\n",
    "correct_answers = 0\n",
    "total_parent_nodes = 0\n",
    "for example in test_data_musique:\n",
    "    for node in example:\n",
    "        if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "            total_parent_nodes += 1\n",
    "            answer, method = greedy_solver(node, cb_model, ob_model, child_model)\n",
    "            question_text = node.get(\"question_text\", \"\").strip()\n",
    "            final_answer, _ = q2gold[question_text]\n",
    "            # if normalize_answer(answer) == normalize_answer(final_answer):\n",
    "            if are_answers_equivalent_using_llm(final_answer, answer):\n",
    "                correct_answers += 1\n",
    "            results.append({\n",
    "                \"idx\": node[\"idx\"],\n",
    "                \"question\": node[\"question_text\"],\n",
    "                \"answer\": answer,\n",
    "                \"method\": method\n",
    "            })\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Node {result['idx']}:\")\n",
    "    print(f\"  Question: {result['question']}\")\n",
    "    print(f\"  Answer: {result['answer']}\")\n",
    "    print(f\"  Method: {result['method']}\")\n",
    "    print()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "print(f\"Times answer matches gold answer: {correct_answers}\")\n",
    "print(f\"Greedy Solver match rate: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling a tree from a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tree_Generation.tree_resampling import TreeResamplingPipeline\n",
    "# tree_resampling_pipeline = TreeResamplingPipeline()\n",
    "tree_resampling_pipeline = TreeResamplingPipeline(use_cache=True)\n",
    "question = \"what is the population size of the smallest city in the world?\"\n",
    "tree_resampling_pipeline.resample_tree(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the state space, action space, and Q-table\n",
    "action_space = ['CB', 'OB', 'Child']  # Possible actions\n",
    "Q = defaultdict(lambda: np.zeros(len(action_space)))  # Q-table\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.98\n",
    "epsilon_min = 0.01\n",
    "\n",
    "# Initialize global counters for success rates\n",
    "success_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0}\n",
    "attempt_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0}\n",
    "\n",
    "# Define the reward function\n",
    "def get_reward(chosen_answer, gold_answer):\n",
    "    # if normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "    if are_answers_equivalent_using_llm(gold_answer, chosen_answer):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Function to update success rates dynamically\n",
    "def update_success_rate(answer_type, is_correct):\n",
    "    \"\"\"\n",
    "    Update the success rate for a given answer type.\n",
    "    :param answer_type: \"cb\", \"ob\", or \"child\"\n",
    "    :param is_correct: True if the answer was correct, False otherwise\n",
    "    \"\"\"\n",
    "    global success_counts, attempt_counts\n",
    "    answer_type = answer_type.lower()\n",
    "    attempt_counts[answer_type] += 1\n",
    "    if is_correct:\n",
    "        success_counts[answer_type] += 1\n",
    "\n",
    "def get_success_rate(answer_type):\n",
    "    \"\"\"\n",
    "    Get the current success rate for a given answer type.\n",
    "    :param answer_type: \"cb\", \"ob\", or \"child\"\n",
    "    :return: Success rate (float)\n",
    "    \"\"\"\n",
    "    global success_counts, attempt_counts\n",
    "    if attempt_counts[answer_type] == 0:\n",
    "        # Default success rates (can be adjusted based on prior knowledge)\n",
    "        default_rates = {\"cb\": 0.32, \"ob\": 0.32, \"child\": 0.42}\n",
    "        return default_rates[answer_type]\n",
    "    return success_counts[answer_type] / attempt_counts[answer_type]\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 3 = 1152\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    # question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    # cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 2)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 2)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    # child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        cb_logprob +  # CB log probabilities\n",
    "        ob_logprob +  # OB log probabilities\n",
    "        child_logprob + # Child log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        # list(question_embedding) +  # Semantic embedding of the question\n",
    "        # list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        # list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        [cb_confidence, ob_confidence, child_confidence] +  # Confidence scores for CB and OB\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length]  # Answer quality features\n",
    "    )\n",
    "    return state\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning(node, gold_answer):\n",
    "    global epsilon\n",
    "    state = tuple(get_state(node))  # Convert state to tuple\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = random.choice(action_space)  # Explore action space\n",
    "    else:\n",
    "        # action = action_space[np.argmax(Q[state])]  # Exploit learned values\n",
    "        max_q_value = np.max(Q[state])  # Find the maximum Q-value\n",
    "        best_actions = [action_space[i] for i, q in enumerate(Q[state]) if q == max_q_value]  # Get all actions with max Q-value\n",
    "        action = np.random.choice(best_actions)  # Choose randomly among them\n",
    "\n",
    "\n",
    "    # Simulate the action (choose answer based on action)\n",
    "    if action == 'CB':\n",
    "        answer = node.get(\"cb_answer\")[0]\n",
    "    elif action == 'OB':\n",
    "        answer = node.get(\"ob_answer\")[0]\n",
    "    else:\n",
    "        answer = node.get(\"child_answer\")[0]\n",
    "\n",
    "    # Compute reward\n",
    "    reward = get_reward(answer, gold_answer)\n",
    "    next_state = tuple(get_state(node))  # Convert next_state to tuple\n",
    "\n",
    "    # Update Q-value\n",
    "    old_value = Q[state][action_space.index(action)]\n",
    "    next_max = np.max(Q[next_state])\n",
    "    Q[state][action_space.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "    # Update success rate for the chosen action\n",
    "    is_correct = (reward == 1)\n",
    "    update_success_rate(action.lower(), is_correct)\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    return answer, action\n",
    "\n",
    "\n",
    "\n",
    "def encode_question_type(question_text):\n",
    "    # Example: Encode question type based on the first word\n",
    "    first_word = question_text.strip().split()[0].lower()\n",
    "    if first_word == \"what\":\n",
    "        return 0\n",
    "    elif first_word == \"where\":\n",
    "        return 1\n",
    "    elif first_word == \"how\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 3  # Other\n",
    "\n",
    "# Hyperparameters for multiple iterations\n",
    "num_iterations = 2000  # Number of iterations over the dataset\n",
    "\n",
    "max_percentage, max_correct = 0, None\n",
    "best_Q = None\n",
    "\n",
    "# Run the Q-learning algorithm for multiple iterations\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "    results = []\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                total_parent_nodes += 1\n",
    "                question_text = node.get(\"question_text\", \"\").strip()\n",
    "                final_answer, _ = q2gold[question_text]\n",
    "                answer, method = q_learning(node, final_answer)\n",
    "                # if normalize_answer(answer) == normalize_answer(final_answer):\n",
    "                if are_answers_equivalent_using_llm(final_answer, answer):\n",
    "                    correct_answers += 1\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": answer,\n",
    "                    \"method\": method\n",
    "                })\n",
    "\n",
    "    # Print the results for this iteration\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    max_percentage = max(max_percentage, accuracy)\n",
    "    if max_percentage == accuracy:\n",
    "        max_correct = correct_answers\n",
    "        best_Q = Q.copy()\n",
    "\n",
    "    print(f\"Total parent nodes: {total_parent_nodes}\")\n",
    "    print(f\"Times answer matches gold answer: {correct_answers}\")\n",
    "    print(f\"Q-learning Solver match rate: {accuracy:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_counts, success_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_percentage, max_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for val in Q.values():\n",
    "    # print action\n",
    "    print(action_space[np.argmax(val)])\n",
    "    actions.append(action_space[np.argmax(val)])\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(actions, bins=3, discrete=True)\n",
    "plt.title(\"Distribution of Maximum Actions\")\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Function to select an action using the trained Q-table (no exploration)\n",
    "def select_action(state):\n",
    "    \"\"\" Select the best action using the trained Q-values. \"\"\"\n",
    "    q_values = Q[state]\n",
    "    if state[3] == 0:\n",
    "        # no children\n",
    "        q_values[2] = -float('inf')\n",
    "    max_q_value = np.max(Q[state])  # Get max Q-value\n",
    "    best_actions = [action_space[i] for i, q in enumerate(Q[state]) if q == max_q_value]  # Get all actions with max Q-value\n",
    "    print(\"best_actions\", best_actions)\n",
    "    return np.random.choice(best_actions)  # Choose randomly if multiple exist\n",
    "\n",
    "# Function to make predictions on test data\n",
    "def run_inference(test_data_hotpotqa):\n",
    "    \"\"\" Run inference on the test data using the trained Q-table. \"\"\"\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    \n",
    "    for example in test_data_hotpotqa:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                final_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                print(node[\"question_text\"])\n",
    "                state = tuple(get_state(node))  # Convert to tuple for Q-table lookup\n",
    "                action = select_action(state)  # Use trained Q-table\n",
    "\n",
    "                # Choose the answer based on the selected action\n",
    "                if action == 'CB':\n",
    "                    answer = node.get(\"cb_answer\")[0]\n",
    "                elif action == 'OB':\n",
    "                    answer = node.get(\"ob_answer\")[0]\n",
    "                else:\n",
    "                    answer = node.get(\"child_answer\")[0]\n",
    "\n",
    "                if \"unknown\" in answer.lower().strip():\n",
    "                    action = \"CB\"\n",
    "                    answer = node.get(\"cb_answer\")[0]\n",
    "                    \n",
    "                total_parent_nodes += 1\n",
    "                \n",
    "                print(\"final_answer\", final_answer)\n",
    "                print(\"answer\", answer)\n",
    "                # if normalize_answer(answer) == normalize_answer(final_answer):\n",
    "                if are_answers_equivalent_using_llm(final_answer, answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": answer,\n",
    "                    \"gold\": final_answer,\n",
    "                    \"method\": action\n",
    "                })\n",
    "\n",
    "    return results, correct_answers, total_parent_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_after_resolving_references(node, tree):\n",
    "    question = node[\"question_text\"].strip()\n",
    "    ref_tokens = re.findall(r\"<\\d+>\", question)\n",
    "    topic_entities = []\n",
    "    # print(\"question, ref_tokens\", question, ref_tokens)\n",
    "    # return\n",
    "    for ref_token in ref_tokens:\n",
    "        if \"fa\" in node and int(ref_token[1:-1]) <= len(tree[node[\"fa\"]][\"sons\"]):\n",
    "            ref_idx = tree[node[\"fa\"]][\"sons\"][int(ref_token[1:-1])-1]\n",
    "            # print(\"ref_idx\", ref_idx)\n",
    "            if \"answer\" in tree[ref_idx]:\n",
    "                question = question.replace(ref_token, tree[ref_idx][\"answer\"][0])\n",
    "                topic_entities.append(tree[ref_idx][\"answer\"][0])\n",
    "\n",
    "    node[\"question\"] = question\n",
    "    return question, topic_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Load test data\n",
    "with open('results-testset-hotpotqa.json', 'r') as file:\n",
    "    test_data_hotpotqa = json.load(file)\n",
    "with open('results-testset-2wiki.json', 'r') as file:\n",
    "    test_data_2wiki = json.load(file)\n",
    "with open('results-testset-musique.json', 'r') as file:\n",
    "    test_data_musique = json.load(file)\n",
    "raw_data_hotpotqa = [json.loads(line.strip()) for line in open('./hotpotqa__v2_test_random_500.jsonl')]\n",
    "\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_test_random_500.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_test_random_500.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "\n",
    "q2gold_test_hotpotqa = {}\n",
    "for item in raw_data_hotpotqa:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold_test_hotpotqa[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "\n",
    "q2gold_test_2wiki = {}\n",
    "for item in raw_data_2wiki:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold_test_2wiki[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "\n",
    "q2gold_test_musique = {}\n",
    "for item in raw_data_musique:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q2gold_test_musique[question] = (gold, item['dataset'])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR CASE\", e)\n",
    "\n",
    "print(\"q2gold_test_hotpotqa\", len(q2gold_test_hotpotqa))\n",
    "print(\"q2gold_test_2wiki\", len(q2gold_test_2wiki))\n",
    "print(\"q2gold_test_musique\", len(q2gold_test_musique))\n",
    "print(\"raw_data_hotpotqa\", len(raw_data_hotpotqa))\n",
    "print(\"raw_data_2wiki\", len(raw_data_2wiki))\n",
    "print(\"raw_data_musique\", len(raw_data_musique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach with deep learning using question only in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# def get_state(node, depth=0):\n",
    "#     # Extract raw features\n",
    "#     cb_logprob = node.get(\"cb_answer\", [None, None])[1] or -100\n",
    "#     ob_logprob = node.get(\"ob_answer\", [None, None])[1] or -100\n",
    "#     child_logprob = node.get(\"child_answer\", [None, None])[1] or -100\n",
    "#     has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "#     question_length = len(node.get(\"question_text\", \"\").split())\n",
    "#     question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "#     num_children = len(node.get(\"sons\", []))\n",
    "#     cb_success_rate = get_success_rate(\"cb\")\n",
    "#     ob_success_rate = get_success_rate(\"ob\")\n",
    "#     child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "#     # Build state vector\n",
    "#     state = [\n",
    "#         cb_logprob,\n",
    "#         ob_logprob,\n",
    "#         # child_logprob,\n",
    "#         has_children,\n",
    "#         question_length,\n",
    "#         question_type,\n",
    "#         num_children,\n",
    "#         cb_success_rate,\n",
    "#         ob_success_rate,\n",
    "#         child_success_rate\n",
    "#     ]\n",
    "#     return torch.FloatTensor(state)\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "# # Define the reward function\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     if normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     # Compute embeddings for the chosen and gold answers\n",
    "#     chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "#     gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "#     # Compute cosine similarity\n",
    "#     similarity = util.cos_sim(chosen_embedding, gold_embedding).item()\n",
    "#     return similarity  # Reward is the similarity score (between -1 and 1)\n",
    "\n",
    "def get_reward(chosen_answer, gold_answer, action, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "    \"\"\"\n",
    "    Reward function with tradeoff between accuracy and efficiency.\n",
    "    :param chosen_answer: Answer chosen by the agent.\n",
    "    :param gold_answer: Ground truth answer.\n",
    "    :param action: The action selected by the agent (0=CB, 1=OB, 2=Child).\n",
    "    :param num_llm_calls: The number of LLM calls made during the current decision.\n",
    "    :param alpha: Weight for accuracy reward.\n",
    "    :param beta: Weight for efficiency penalty.\n",
    "    :return: A reward value that balances accuracy and efficiency.\n",
    "    \"\"\"\n",
    "    # Compute Accuracy Reward\n",
    "    chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "    gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(chosen_embedding, gold_embedding).item()  # Value between -1 (opposite) and 1 (exact match)\n",
    "    accuracy_reward = max(0, similarity)  # Ensure rewards are non-negative\n",
    "\n",
    "    # Define LLM cost for each action\n",
    "    ACTION_COSTS = {\n",
    "        0: 1,  # CB cost\n",
    "        1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "        2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "    }\n",
    "\n",
    "    # Compute Efficiency Penalty\n",
    "    action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "    efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "    # Combine Accuracy and Efficiency\n",
    "    reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    # child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    # child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 3 = 1152\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 2)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 2)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    # child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        # cb_logprob +  # CB log probabilities\n",
    "        # ob_logprob +  # OB log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        list(question_embedding) +  # Semantic embedding of the question\n",
    "        # list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        # list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        [cb_confidence, ob_confidence] +  # Confidence scores for CB and OB\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length]  # Answer quality features\n",
    "    )\n",
    "\n",
    "    # maybe try question only by masking all and add log probs for the solved one and try adding verification step in the end.\n",
    "\n",
    "    return torch.FloatTensor(state)\n",
    "\n",
    "HAS_CHILDREN_INDEX = 0\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            if state[HAS_CHILDREN_INDEX] == 0: # has children = 0, because child is not possible\n",
    "                return random.randint(0, len(action_space) - 2)\n",
    "            else:\n",
    "                return random.randint(0, len(action_space) - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                if state[HAS_CHILDREN_INDEX] == 0: # has children = 0\n",
    "                    q_values[-1] = -float('inf')\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # update the success rates\n",
    "        for i, action in enumerate(actions):\n",
    "            if rewards[i] == 1:\n",
    "                update_success_rate(action_space[action], True)\n",
    "            else:\n",
    "                update_success_rate(action_space[action], False)\n",
    "\n",
    "        # Compute loss and update the network\n",
    "        # loss = nn.MSELoss()(q_values.squeeze(), target_q_values)\n",
    "        loss = nn.SmoothL1Loss()(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs(\"saved_models_question_only_state_trained_all\", exist_ok=True)\n",
    "\n",
    "# Define different configurations for alpha and beta\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "accuracy_rewards_numcalls = {}\n",
    "\n",
    "# Loop through different settings\n",
    "for config in configurations:\n",
    "    alpha = config[\"alpha\"]\n",
    "    beta = config[\"beta\"]\n",
    "    config_name = config[\"name\"]\n",
    "    state_dim = 7 + 384 + 2 + 2 + 2  # 100 + 7 + 1152 + 2 + 2 + 2 # Number of features in the state vector\n",
    "    action_dim = len(action_space)  # Number of actions (CB, OB, Child)\n",
    "    hidden_dim = 128  # Hidden layer size\n",
    "    lr = 5e-3  # Increase from 1e-3  //     1e-3  # Learning rate\n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    epsilon_decay = 0.98  # Decay rate for exploration\n",
    "    batch_size = 128  # Mini-batch size\n",
    "    num_episodes = 20  # Number of training episodes\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "\n",
    "    print(f\"\\nTraining with {config_name} (alpha={alpha}, beta={beta})\")\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_agent = None\n",
    "\n",
    "    # Store rewards, accuracy and total number of LLM calls for each episode\n",
    "    rewards_list = []\n",
    "    accuracy_list = []\n",
    "    total_num_llm_calls_list = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    agent.q_network.train()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        total_reward = 0\n",
    "        correct_answers = 0\n",
    "        total_parent_nodes = 0\n",
    "        total_num_llm_calls = 0\n",
    "\n",
    "        for example in data:\n",
    "            for node in example:\n",
    "                if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                    gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                    total_parent_nodes += 1\n",
    "                    state = get_state(node)  \n",
    "                    action = agent.select_action(state, epsilon)  \n",
    "                    chosen_answer = None\n",
    "                    fallback_used = False  \n",
    "                    num_llm_calls = None\n",
    "\n",
    "                    question, _ = get_question_after_resolving_references(node, example)\n",
    "\n",
    "                    # Simulate the action\n",
    "                    if action == 0: \n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 1:\n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 2:\n",
    "                        tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                        child_experiences = []\n",
    "\n",
    "                        for node_ in tree_with_answers_chosen_by_agent:\n",
    "                            question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "                        \n",
    "                            if len(node_[\"sons\"]) == 0:\n",
    "                                child_state = get_state(node_, depth=1)\n",
    "                                child_action = agent.select_action(child_state, epsilon)\n",
    "                                if child_action == 0:  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 1  \n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                elif child_action == 1:  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 0  \n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "\n",
    "                                node_[\"answer\"] = child_answer\n",
    "                                child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "                            else:\n",
    "                                node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                                chosen_answer = node_[\"child_answer\"]\n",
    "                        \n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation)\n",
    "\n",
    "                    if \"unknown\" in chosen_answer[0].lower().strip():\n",
    "                        fallback_used = True\n",
    "                        if action == 0:  \n",
    "                            fallback_action = 1\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        elif action == 1:  \n",
    "                            fallback_action = 0\n",
    "                            node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        elif action == 2:  \n",
    "                            fallback_action = 0\n",
    "                            node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "\n",
    "                    # Compute reward\n",
    "                    reward = get_reward(chosen_answer[0], gold_answer, action, num_llm_calls, alpha, beta)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    # if normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                    if are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    next_state = get_state(node)\n",
    "\n",
    "                    if not fallback_used:\n",
    "                        agent.replay_buffer.append((state, action, reward, next_state, False))  \n",
    "                    else:\n",
    "                        agent.replay_buffer.append((state, fallback_action, reward, next_state, False))\n",
    "\n",
    "                    if action == 2:\n",
    "                        num_children = len(node.get(\"sons\", []))\n",
    "                        child_reward = reward / num_children  \n",
    "                        for child_state, child_action, _, next_child_state, done in child_experiences:\n",
    "                            agent.replay_buffer.append((child_state, child_action, reward, next_child_state, done))\n",
    "\n",
    "                    # update total number of llm calls in this episode\n",
    "                    total_num_llm_calls += num_llm_calls\n",
    "\n",
    "                    agent.train(batch_size)\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "        rewards_list.append(total_reward)\n",
    "        accuracy_list.append(accuracy)\n",
    "        total_num_llm_calls_list.append(total_num_llm_calls)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_agent = copy.deepcopy(agent)\n",
    "\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Total LLM Calls in this episode: {total_num_llm_calls}\")\n",
    "        print(f\"Epsilon: {epsilon:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Store the rewards and accuracy for this configuration\n",
    "    accuracy_rewards_numcalls[config_name] = {\n",
    "        \"rewards\": rewards_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"num_calls\": total_num_llm_calls_list\n",
    "    }\n",
    "\n",
    "    # Save model after training with each configuration\n",
    "    model_path = f\"saved_models_question_only_state_trained_all/agent_{config_name}.pth\"\n",
    "    torch.save(agent.q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the best model overall\n",
    "    if best_agent is not None:\n",
    "        best_model_path = f\"saved_models_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "        torch.save(best_agent.q_network.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get rewards, accuracy and number of LLM calls for each configuration\n",
    "balanced_rewards = accuracy_rewards_numcalls[\"Balanced\"][\"rewards\"]\n",
    "balanced_accuracy = accuracy_rewards_numcalls[\"Balanced\"][\"accuracy\"]\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"]\n",
    "\n",
    "efficiency_rewards = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"rewards\"]\n",
    "efficiency_accuracy = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"accuracy\"]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"]\n",
    "\n",
    "high_accuracy_accuracy = accuracy_rewards_numcalls[\"High_Accuracy\"][\"accuracy\"]\n",
    "high_accuracy_rewards = accuracy_rewards_numcalls[\"High_Accuracy\"][\"rewards\"]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"]\n",
    "\n",
    "# min max scaling for rewards\n",
    "balanced_rewards = (balanced_rewards - np.min(balanced_rewards)) / (np.max(balanced_rewards) - np.min(balanced_rewards))\n",
    "efficiency_rewards = (efficiency_rewards - np.min(efficiency_rewards)) / (np.max(efficiency_rewards) - np.min(efficiency_rewards))\n",
    "high_accuracy_rewards = (high_accuracy_rewards - np.min(high_accuracy_rewards)) / (np.max(high_accuracy_rewards) - np.min(high_accuracy_rewards))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(balanced_rewards, label=\"Balanced\")\n",
    "plt.plot(efficiency_rewards, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_rewards, label=\"High Accuracy\")\n",
    "plt.title(\"Normalized Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Normalized Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(balanced_accuracy, label=\"Balanced\")\n",
    "plt.plot(efficiency_accuracy, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_accuracy, label=\"High Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(balanced_num_calls, label=\"Balanced\")\n",
    "plt.plot(efficiency_num_calls, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_num_calls, label=\"High Accuracy\")\n",
    "plt.title(\"Number of LLM Calls\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Calls\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of calls for each configuration\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"][-1]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"][-1]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"][-1]\n",
    "\n",
    "print(\"balanced_num_calls\", balanced_num_calls)\n",
    "print(\"efficiency_num_calls\", efficiency_num_calls)\n",
    "print(\"high_accuracy_num_calls\", high_accuracy_num_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent on the test data\n",
    "def evaluate_agent(agent, data, q2gold):\n",
    "    agent.q_network.eval()  # Set the model to evaluation mode\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                total_parent_nodes += 1\n",
    "                state = get_state(node)\n",
    "                action = agent.select_action(state, epsilon=0)  # No exploration during evaluation\n",
    "                chosen_answer = None\n",
    "                fallback_used = False  # Track if a fallback was used\n",
    "                fallback_action = None\n",
    "\n",
    "                question, _ = get_question_after_resolving_references(node, example)\n",
    "                # Simulate the action (choose answer based on action)\n",
    "                if action == 0:  # CB\n",
    "                    node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to OB\n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                        fallback_action = 1\n",
    "                        fallback_used = True\n",
    "                elif action == 1:  # OB\n",
    "                    node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                        fallback_action = 0\n",
    "                        fallback_used = True\n",
    "                elif action == 2:  # Child\n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                    for child_idx in node.get(\"sons\", []):\n",
    "                        child = tree_with_answers_chosen_by_agent[child_idx]\n",
    "                        child_state = get_state(child, depth=1)\n",
    "                        child_action = agent.select_action(child_state, epsilon=0)\n",
    "                        question_, topic_entities = get_question_after_resolving_references(child, tree_with_answers_chosen_by_agent)\n",
    "                        if child_action == 0:  # CB\n",
    "                            child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                            child_answer = child.get(\"cb_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to OB\n",
    "                                child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = child.get(\"ob_answer\", [None])\n",
    "                        elif child_action == 1:  # OB\n",
    "                            child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                            child_answer = child.get(\"ob_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to CB\n",
    "                                child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = child.get(\"cb_answer\", [None])\n",
    "\n",
    "                        tree_with_answers_chosen_by_agent[child_idx][\"answer\"] = child_answer\n",
    "\n",
    "                    # Generate child_answer for the parent node\n",
    "                    # print(\"tree_with_answers_chosen_by_agent\", tree_with_answers_chosen_by_agent)\n",
    "                    node[\"child_answer\"], node[\"answer\"] = aggregate_multihop_answer(node, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                    chosen_answer = node[\"child_answer\"][0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB or OB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]  # Try CB first\n",
    "                        # fallback_used = True\n",
    "                        if \"unknown\" in chosen_answer.lower().strip():\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])[0]  # Try OB next\n",
    "\n",
    "                # Compute reward\n",
    "                # if chosen_answer and normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(chosen_answer, gold_answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": chosen_answer,\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "# List of trained models to evaluate\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach with deep learning (using Question + cb + ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "# # Define the reward function\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     if normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     # Compute embeddings for the chosen and gold answers\n",
    "#     chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "#     gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "#     # Compute cosine similarity\n",
    "#     similarity = util.cos_sim(chosen_embedding, gold_embedding).item()\n",
    "#     return similarity  # Reward is the similarity score (between -1 and 1)\n",
    "\n",
    "def get_reward(chosen_answer, gold_answer, action, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "    \"\"\"\n",
    "    Reward function with tradeoff between accuracy and efficiency.\n",
    "    :param chosen_answer: Answer chosen by the agent.\n",
    "    :param gold_answer: Ground truth answer.\n",
    "    :param action: The action selected by the agent (0=CB, 1=OB, 2=Child).\n",
    "    :param num_llm_calls: The number of LLM calls made during the current decision.\n",
    "    :param alpha: Weight for accuracy reward.\n",
    "    :param beta: Weight for efficiency penalty.\n",
    "    :return: A reward value that balances accuracy and efficiency.\n",
    "    \"\"\"\n",
    "    # Compute Accuracy Reward\n",
    "    chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "    gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(chosen_embedding, gold_embedding).item()  # Value between -1 (opposite) and 1 (exact match)\n",
    "    accuracy_reward = (similarity + 1) / 2  # Normalize to range [0, 1]\n",
    "\n",
    "    # Define LLM cost for each action\n",
    "    ACTION_COSTS = {\n",
    "        0: 1,  # CB cost\n",
    "        1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "        2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "    }\n",
    "\n",
    "    # Compute Efficiency Penalty\n",
    "    action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "    efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "    # Combine Accuracy and Efficiency\n",
    "    reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    # child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    # child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 3 = 1152\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 2)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 2)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    # child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        cb_logprob +  # CB log probabilities\n",
    "        ob_logprob +  # OB log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        list(question_embedding) +  # Semantic embedding of the question\n",
    "        list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        [cb_confidence, ob_confidence] +  # Confidence scores for CB and OB\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length]  # Answer quality features\n",
    "    )\n",
    "\n",
    "    # maybe try question only by masking all and add log probs for the solved one and try adding verification step in the end.\n",
    "\n",
    "    return torch.FloatTensor(state)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            if state[100] == 0: # has children = 0, because child is not possible\n",
    "                return random.randint(0, len(action_space) - 2)\n",
    "            else:\n",
    "                return random.randint(0, len(action_space) - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                if state[100] == 0: # has children = 0\n",
    "                    q_values[-1] = -float('inf')\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # update the success rates\n",
    "        for i, action in enumerate(actions):\n",
    "            if rewards[i] == 1:\n",
    "                update_success_rate(action_space[action], True)\n",
    "            else:\n",
    "                update_success_rate(action_space[action], False)\n",
    "\n",
    "        # Compute loss and update the network\n",
    "        # loss = nn.MSELoss()(q_values.squeeze(), target_q_values)\n",
    "        loss = nn.SmoothL1Loss()(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs(\"saved_models_trained_all\", exist_ok=True)\n",
    "\n",
    "# Define different configurations for alpha and beta\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "accuracy_rewards_numcalls = {}\n",
    "\n",
    "# Loop through different settings\n",
    "for config in configurations:\n",
    "    alpha = config[\"alpha\"]\n",
    "    beta = config[\"beta\"]\n",
    "    config_name = config[\"name\"]\n",
    "    state_dim = 100 + 7 + 1152 + 2 + 2 + 2 # Number of features in the state vector\n",
    "    action_dim = len(action_space)  # Number of actions (CB, OB, Child)\n",
    "    hidden_dim = 128  # Hidden layer size\n",
    "    lr = 5e-3  # Increase from 1e-3  //     1e-3  # Learning rate\n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    epsilon_decay = 0.98  # Decay rate for exploration\n",
    "    batch_size = 128  # Mini-batch size\n",
    "    num_episodes = 20  # Number of training episodes\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "\n",
    "    print(f\"\\nTraining with {config_name} (alpha={alpha}, beta={beta})\")\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_agent = None\n",
    "\n",
    "    # Store rewards, accuracy and total number of LLM calls for each episode\n",
    "    rewards_list = []\n",
    "    accuracy_list = []\n",
    "    total_num_llm_calls_list = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    agent.q_network.train()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        total_reward = 0\n",
    "        correct_answers = 0\n",
    "        total_parent_nodes = 0\n",
    "        total_num_llm_calls = 0\n",
    "\n",
    "        for example in data:\n",
    "            for node in example:\n",
    "                if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                    gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                    total_parent_nodes += 1\n",
    "                    state = get_state(node)  \n",
    "                    action = agent.select_action(state, epsilon)  \n",
    "                    chosen_answer = None\n",
    "                    fallback_used = False  \n",
    "                    num_llm_calls = None\n",
    "\n",
    "                    question, _ = get_question_after_resolving_references(node, example)\n",
    "                    # Simulate the action\n",
    "                    if action == 0:\n",
    "                        node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        # chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 1:\n",
    "                        # since you are the parent, so topic entities are empty because no references\n",
    "                        node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 2:\n",
    "                        tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                        child_experiences = []\n",
    "\n",
    "                        for node_ in tree_with_answers_chosen_by_agent:\n",
    "                            question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "                            if len(node_[\"sons\"]) == 0:\n",
    "                                child_state = get_state(node_, depth=1)\n",
    "                                child_action = agent.select_action(child_state, epsilon)\n",
    "                                if child_action == 0:  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 1  \n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                elif child_action == 1:  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 0  \n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "\n",
    "                                node_[\"answer\"] = child_answer\n",
    "                                child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "                            else:\n",
    "                                node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                                chosen_answer = node_[\"child_answer\"]\n",
    "                        \n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation)\n",
    "\n",
    "                    if \"unknown\" in chosen_answer[0].lower().strip():\n",
    "                        fallback_used = True\n",
    "                        if action == 0:  \n",
    "                            fallback_action = 1\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        elif action == 1:  \n",
    "                            fallback_action = 0\n",
    "                            node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        elif action == 2:  \n",
    "                            fallback_action = 0\n",
    "                            node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "\n",
    "                    # Compute reward\n",
    "                    reward = get_reward(chosen_answer[0], gold_answer, action, num_llm_calls, alpha, beta)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    # if normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                    if are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    next_state = get_state(node)\n",
    "\n",
    "                    if not fallback_used:\n",
    "                        agent.replay_buffer.append((state, action, reward, next_state, False))  \n",
    "                    else:\n",
    "                        agent.replay_buffer.append((state, fallback_action, reward, next_state, False))\n",
    "\n",
    "                    if action == 2:\n",
    "                        num_children = len(node.get(\"sons\", []))\n",
    "                        child_reward = reward / num_children  \n",
    "                        for child_state, child_action, _, next_child_state, done in child_experiences:\n",
    "                            agent.replay_buffer.append((child_state, child_action, reward, next_child_state, done))\n",
    "\n",
    "                    # update total number of llm calls in this episode\n",
    "                    total_num_llm_calls += num_llm_calls\n",
    "\n",
    "                    agent.train(batch_size)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "        rewards_list.append(total_reward)\n",
    "        accuracy_list.append(accuracy)\n",
    "        total_num_llm_calls_list.append(total_num_llm_calls)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_agent = copy.deepcopy(agent)\n",
    "\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Total LLM Calls in this episode: {total_num_llm_calls}\")\n",
    "        print(f\"Epsilon: {epsilon:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Store the rewards and accuracy for this configuration\n",
    "    accuracy_rewards_numcalls[config_name] = {\n",
    "        \"rewards\": rewards_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"num_calls\": total_num_llm_calls_list\n",
    "    }\n",
    "\n",
    "    # Save model after training with each configuration\n",
    "    model_path = f\"saved_models_trained_all/agent_{config_name}.pth\"\n",
    "    torch.save(agent.q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the best model overall\n",
    "    if best_agent is not None:\n",
    "        best_model_path = f\"saved_models_trained_all/best_agent_{config_name}.pth\"\n",
    "        torch.save(best_agent.q_network.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get rewards, accuracy and number of LLM calls for each configuration\n",
    "balanced_rewards = accuracy_rewards_numcalls[\"Balanced\"][\"rewards\"]\n",
    "balanced_accuracy = accuracy_rewards_numcalls[\"Balanced\"][\"accuracy\"]\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"]\n",
    "\n",
    "efficiency_rewards = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"rewards\"]\n",
    "efficiency_accuracy = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"accuracy\"]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"]\n",
    "\n",
    "high_accuracy_accuracy = accuracy_rewards_numcalls[\"High_Accuracy\"][\"accuracy\"]\n",
    "high_accuracy_rewards = accuracy_rewards_numcalls[\"High_Accuracy\"][\"rewards\"]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"]\n",
    "\n",
    "# min max scaling for rewards\n",
    "balanced_rewards = (balanced_rewards - np.min(balanced_rewards)) / (np.max(balanced_rewards) - np.min(balanced_rewards))\n",
    "efficiency_rewards = (efficiency_rewards - np.min(efficiency_rewards)) / (np.max(efficiency_rewards) - np.min(efficiency_rewards))\n",
    "high_accuracy_rewards = (high_accuracy_rewards - np.min(high_accuracy_rewards)) / (np.max(high_accuracy_rewards) - np.min(high_accuracy_rewards))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(balanced_rewards, label=\"Balanced\")\n",
    "plt.plot(efficiency_rewards, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_rewards, label=\"High Accuracy\")\n",
    "plt.title(\"Normalized Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Normalized Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(balanced_accuracy, label=\"Balanced\")\n",
    "plt.plot(efficiency_accuracy, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_accuracy, label=\"High Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(balanced_num_calls, label=\"Balanced\")\n",
    "plt.plot(efficiency_num_calls, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_num_calls, label=\"High Accuracy\")\n",
    "plt.title(\"Number of LLM Calls\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Calls\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of calls for each configuration\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"][-1]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"][-1]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"][-1]\n",
    "\n",
    "print(\"balanced_num_calls\", balanced_num_calls)\n",
    "print(\"efficiency_num_calls\", efficiency_num_calls)\n",
    "print(\"high_accuracy_num_calls\", high_accuracy_num_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent on the test data\n",
    "def evaluate_agent(agent, data, q2gold):\n",
    "    agent.q_network.eval()  # Set the model to evaluation mode\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                total_parent_nodes += 1\n",
    "                state = get_state(node)\n",
    "                action = agent.select_action(state, epsilon=0)  # No exploration during evaluation\n",
    "                chosen_answer = None\n",
    "                fallback_used = False  # Track if a fallback was used\n",
    "                fallback_action = None\n",
    "\n",
    "                question, _ = get_question_after_resolving_references(node, example)\n",
    "                # Simulate the action (choose answer based on action)\n",
    "                if action == 0:  # CB\n",
    "                    node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to OB\n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                        fallback_action = 1\n",
    "                        fallback_used = True\n",
    "                elif action == 1:  # OB\n",
    "                    node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                        fallback_action = 0\n",
    "                        fallback_used = True\n",
    "                elif action == 2:  # Child\n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                    for child_idx in node.get(\"sons\", []):\n",
    "                        child = tree_with_answers_chosen_by_agent[child_idx]\n",
    "                        child_state = get_state(child, depth=1)\n",
    "                        child_action = agent.select_action(child_state, epsilon=0)\n",
    "                        question_, topic_entities = get_question_after_resolving_references(child, tree_with_answers_chosen_by_agent)\n",
    "                        if child_action == 0:  # CB\n",
    "                            child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                            child_answer = child.get(\"cb_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to OB\n",
    "                                child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = child.get(\"ob_answer\", [None])\n",
    "                        elif child_action == 1:  # OB\n",
    "                            child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                            child_answer = child.get(\"ob_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to CB\n",
    "                                child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = child.get(\"cb_answer\", [None])\n",
    "\n",
    "                        tree_with_answers_chosen_by_agent[child_idx][\"answer\"] = child_answer\n",
    "\n",
    "                    # Generate child_answer for the parent node\n",
    "                    # print(\"tree_with_answers_chosen_by_agent\", tree_with_answers_chosen_by_agent)\n",
    "                    node[\"child_answer\"], node[\"answer\"] = aggregate_multihop_answer(node, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                    chosen_answer = node[\"child_answer\"][0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB or OB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]  # Try CB first\n",
    "                        # fallback_used = True\n",
    "                        if \"unknown\" in chosen_answer.lower().strip():\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])[0]  # Try OB next\n",
    "\n",
    "                # Compute reward\n",
    "                # if chosen_answer and normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(chosen_answer, gold_answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": chosen_answer,\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "# List of trained models to evaluate\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach with deep learning (using Question + cb + ob + Reformulation of question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ['CB', 'OB', 'Child', 'CB_REFORMULATE', 'OB_REFORMULATE', 'Child_REFORMULATE']\n",
    "success_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0, \"cb_reformulate\": 0, \"ob_reformulate\": 0, \"child_reformulate\": 0}\n",
    "attempt_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0, \"cb_reformulate\": 0, \"ob_reformulate\": 0, \"child_reformulate\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "# # Define the reward function\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     if normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     # Compute embeddings for the chosen and gold answers\n",
    "#     chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "#     gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "#     # Compute cosine similarity\n",
    "#     similarity = util.cos_sim(chosen_embedding, gold_embedding).item()\n",
    "#     return similarity  # Reward is the similarity score (between -1 and 1)\n",
    "\n",
    "def get_reward(chosen_answer, gold_answer, action, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "    \"\"\"\n",
    "    Reward function with tradeoff between accuracy and efficiency.\n",
    "    :param chosen_answer: Answer chosen by the agent.\n",
    "    :param gold_answer: Ground truth answer.\n",
    "    :param action: The action selected by the agent (0=CB, 1=OB, 2=Child).\n",
    "    :param num_llm_calls: The number of LLM calls made during the current decision.\n",
    "    :param alpha: Weight for accuracy reward.\n",
    "    :param beta: Weight for efficiency penalty.\n",
    "    :return: A reward value that balances accuracy and efficiency.\n",
    "    \"\"\"\n",
    "    # Compute Accuracy Reward\n",
    "    chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "    gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(chosen_embedding, gold_embedding).item()  # Value between -1 (opposite) and 1 (exact match)\n",
    "    accuracy_reward = max(0, similarity)  # Ensure rewards are non-negative\n",
    "\n",
    "    # Define LLM cost for each action\n",
    "    ACTION_COSTS = {\n",
    "        0: 1,  # CB cost\n",
    "        1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "        2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "        3: 1,  # CB Reformulation cost\n",
    "        4: 1,  # OB Reformulation cost\n",
    "        5: 1   # Child Reformulation cost\n",
    "    }\n",
    "\n",
    "    # Compute Efficiency Penalty\n",
    "    action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "    efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "    # Combine Accuracy and Efficiency\n",
    "    reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    # child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    # child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 3 = 1152\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 2)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 2)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    # child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        cb_logprob +  # CB log probabilities\n",
    "        ob_logprob +  # OB log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        list(question_embedding) +  # Semantic embedding of the question\n",
    "        list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        [cb_confidence, ob_confidence] +  # Confidence scores for CB and OB\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length]  # Answer quality features\n",
    "    )\n",
    "\n",
    "    # maybe try question only by masking all and add log probs for the solved one and try adding verification step in the end.\n",
    "\n",
    "    return torch.FloatTensor(state)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            if state[100] == 0:  # has children = 0, meaning child actions are not possible\n",
    "                return random.choice([0, 1, 3, 4])  # Exclude actions 2 and 5\n",
    "            else:\n",
    "                return random.randint(0, 5)  # Choose from all 6 actions\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state).clone()  # Clone to avoid in-place modification issues\n",
    "                \n",
    "                if state[100] == 0:  # has children = 0\n",
    "                    q_values[2] = -float('inf')  # Mask action 2\n",
    "                    q_values[5] = -float('inf')  # Mask action 5\n",
    "\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # update the success rates\n",
    "        for i, action in enumerate(actions):\n",
    "            if rewards[i] == 1:\n",
    "                update_success_rate(action_space[action], True)\n",
    "            else:\n",
    "                update_success_rate(action_space[action], False)\n",
    "\n",
    "        # Compute loss and update the network\n",
    "        # loss = nn.MSELoss()(q_values.squeeze(), target_q_values)\n",
    "        loss = nn.SmoothL1Loss()(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_question(question):\n",
    "   \"\"\"\n",
    "   Reformulates a question using a language model based on pre-defined reformulation rules.\n",
    "   :param question: The original question to reformulate.\n",
    "   :return: Reformulated question text.\n",
    "   \"\"\"\n",
    "\n",
    "   REFORMULATION_PROMPT = f\"\"\"\n",
    "   Reformulate the question to improve readability, grammar, and clarity. Simplify complex phrasing and structure the question in a way that is easier to understand, without changing the core intent or meaning. Avoid adding or removing any information. Follow these steps:\n",
    "   1. Simplify the question by removing unnecessary or overly complex wording.\n",
    "   2. Fix grammar and punctuation issues to improve readability.\n",
    "   3. Rewrite the question in a way that keeps the meaning identical but makes it easier to interpret.\n",
    "   4. Return the result directly without any additional information.\n",
    "\n",
    "   Examples:\n",
    "\n",
    "   1. Original: \"What are the possible reasons for the decline in monarch butterfly populations, and how does urbanization contribute to this issue?\"\n",
    "      Reformulated: \"What causes the decline in monarch butterfly populations, and how does urbanization play a role?\"\n",
    "\n",
    "   2. Original: \"How is social media influencing the mental health of teenagers, specifically with regard to anxiety and depression levels?\"\n",
    "      Reformulated: \"How does social media affect teenagers' mental health, particularly with respect to anxiety and depression?\"\n",
    "\n",
    "   3. Original: \"What is known about the origin of black holes and how do they affect the galaxies they exist in?\"\n",
    "      Reformulated: \"What do we know about the origin of black holes, and how do they affect their galaxies?\"\n",
    "\n",
    "   4. Original: \"Describe the ways in which renewable energy sources like solar and wind power are replacing fossil fuels in energy production.\"\n",
    "      Reformulated: \"How are renewable energy sources like solar and wind replacing fossil fuels in energy production?\"\n",
    "\n",
    "   Now, reformulate the question below:\n",
    "\n",
    "   Original Question:\n",
    "   {question}\n",
    "\n",
    "   Reformulated Question:\n",
    "   \"\"\"\n",
    "   \n",
    "   # Format the prompt\n",
    "   prompt = REFORMULATION_PROMPT.format(question=question)\n",
    "\n",
    "   # Use LLM API to get the reformulated question\n",
    "   \n",
    "   # TODO:: this should be with temperature 0.7, and dont use cache\n",
    "   response, tag = togetherai_caller.req2provider(prompt=prompt, max_tokens=None, stop= None, use_cache=True)\n",
    "   response = response[0]\n",
    "   return response['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs(\"saved_models_with_reformulation_trained_all\", exist_ok=True)\n",
    "\n",
    "# Define different configurations for alpha and beta\n",
    "configurations = [\n",
    "    {\n",
    "        \"hidden_dim\": 128,\n",
    "        \"lr\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 1.0,\n",
    "        \"epsilon_min\": 0.01,\n",
    "        \"epsilon_decay\": 0.996,\n",
    "        \"batch_size\": 32,\n",
    "        \"num_episodes\": 50,\n",
    "        \"alpha\": 5.0,\n",
    "        \"beta\": 0.05,\n",
    "        \"name\": \"High_Accuracy_Priority\"\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 128,\n",
    "        \"lr\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 1.0,\n",
    "        \"epsilon_min\": 0.01,\n",
    "        \"epsilon_decay\": 0.996,\n",
    "        \"batch_size\": 32,\n",
    "        \"num_episodes\": 50,\n",
    "        \"alpha\": 1.0,\n",
    "        \"beta\": 0.1,\n",
    "        \"name\": \"Balanced_Reward_Tradeoff\"\n",
    "    },\n",
    "    {\n",
    "        \"hidden_dim\": 128,\n",
    "        \"lr\": 5e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 1.0,\n",
    "        \"epsilon_min\": 0.1,\n",
    "        \"epsilon_decay\": 0.996,\n",
    "        \"batch_size\": 32,\n",
    "        \"num_episodes\": 50,\n",
    "        \"alpha\": 0.5,\n",
    "        \"beta\": 2.0,\n",
    "        \"name\": \"Efficiency_Focused\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "accuracy_rewards_numcalls = {}\n",
    "\n",
    "# Loop through different settings\n",
    "for config in configurations:\n",
    "    alpha = config[\"alpha\"]\n",
    "    beta = config[\"beta\"]\n",
    "    \n",
    "    epsilon = config[\"epsilon\"]\n",
    "    epsilon_min = config[\"epsilon_min\"]\n",
    "    epsilon_decay = config[\"epsilon_decay\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_episodes = config[\"num_episodes\"]\n",
    "    lr = config[\"lr\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "    action_dim = len(action_space)\n",
    "\n",
    "    config_name = config[\"name\"]\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "\n",
    "    print(f\"\\nTraining with {config_name} (alpha={alpha}, beta={beta})\")\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_agent = None\n",
    "\n",
    "    # Store rewards, accuracy and total number of LLM calls for each episode\n",
    "    rewards_list = []\n",
    "    accuracy_list = []\n",
    "    total_num_llm_calls_list = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    agent.q_network.train()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        total_reward = 0\n",
    "        correct_answers = 0\n",
    "        total_parent_nodes = 0\n",
    "        total_num_llm_calls = 0\n",
    "\n",
    "        for example in data:\n",
    "            for node in example:\n",
    "                if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                    gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                    total_parent_nodes += 1\n",
    "                    state = get_state(node)  \n",
    "                    action = agent.select_action(state, epsilon)  \n",
    "                    chosen_answer = None\n",
    "                    fallback_used = False  \n",
    "                    num_llm_calls = None\n",
    "\n",
    "                    question, _ = get_question_after_resolving_references(node, example)\n",
    "                    # Simulate the action\n",
    "                    if action == 0:\n",
    "                        node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 1:  \n",
    "                        node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 2:\n",
    "                        tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                        child_experiences = []\n",
    "\n",
    "                        for node_ in tree_with_answers_chosen_by_agent:\n",
    "                            question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "                        \n",
    "                            if len(node_[\"sons\"]) == 0:\n",
    "                                child_state = get_state(node_, depth=1)\n",
    "                                child_action = agent.select_action(child_state, epsilon)\n",
    "                                if child_action == 0:  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 1  \n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                elif child_action == 1:  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 0  \n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                elif child_action == 3:\n",
    "                                    # CB with reformulation\n",
    "                                    reformulated_question = reformulate_question(question_)\n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 4\n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                elif child_action == 4:\n",
    "                                    # OB with reformulation\n",
    "                                    reformulated_question = reformulate_question(question_)\n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 3\n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                else:\n",
    "                                    # Throw exception\n",
    "                                    raise ValueError(\"Invalid action selected for child node : \" + action_space[child_action])\n",
    "\n",
    "                                node_[\"answer\"] = child_answer\n",
    "                                child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "                            else:\n",
    "                                # If the node has children, aggregate the answers\n",
    "                                node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                                chosen_answer = node_[\"child_answer\"]\n",
    "                        \n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation) \n",
    "                        # TODO i am not counting fallbacks here in the children\n",
    "                    elif action == 3:\n",
    "                        # CB with reformulation\n",
    "                        reformulated_question = reformulate_question(question)\n",
    "                        chosen_answer = get_cb_answer(reformulated_question, dataset_used)\n",
    "                        num_llm_calls = 2  # 1 for reformulation, 1 for CB\n",
    "                    \n",
    "                    elif action == 4:\n",
    "                        # OB with reformulation\n",
    "                        reformulated_question = reformulate_question(question)\n",
    "                        chosen_answer = get_singlehop_ob_answer(reformulated_question, [], dataset_used)\n",
    "                        num_llm_calls = 2  # 1 for reformulation, 1 for OB\n",
    "                    \n",
    "                    elif action == 5:\n",
    "                        # Child with reformulation\n",
    "                        tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                        child_experiences = []\n",
    "                        num_llm_calls = 0\n",
    "\n",
    "                        for node_ in tree_with_answers_chosen_by_agent:\n",
    "                            question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "\n",
    "                            if len(node_[\"sons\"]) == 0:\n",
    "                                child_state = get_state(node_, depth=1)\n",
    "                                child_action = agent.select_action(child_state, epsilon)\n",
    "                                # num_llm_calls += 1 # for solving the child\n",
    "                                if child_action == 0:  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 1  \n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                elif child_action == 1:  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 0  \n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                elif child_action == 3:\n",
    "                                    # CB with reformulation\n",
    "                                    reformulated_question = reformulate_question(question_)\n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 4\n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                elif child_action == 4:\n",
    "                                    # OB with reformulation\n",
    "                                    reformulated_question = reformulate_question(question_)\n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 3\n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                        # num_llm_calls += 1 # for fallback\n",
    "                                else:\n",
    "                                    # throw error\n",
    "                                    raise Exception('Wrong action decided for the children ?? ') # Don't! If you catch, likely to hide bugs.\n",
    "\n",
    "\n",
    "                                node_[\"answer\"] = child_answer\n",
    "                                child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "                            else:\n",
    "                                node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                                chosen_answer = node_[\"child_answer\"]\n",
    "                        \n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # 1 (decomposition) + 1 (aggregation)                        \n",
    "\n",
    "                    if \"unknown\" in chosen_answer[0].lower().strip():\n",
    "                        # num_llm_calls += 1  # Fallback to CB or OB\n",
    "                        fallback_used = True\n",
    "                        if action == 0 or action == 3:  \n",
    "                            fallback_action = 1\n",
    "                            node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        elif action == 1 or action == 4:  \n",
    "                            fallback_action = 0\n",
    "                            node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        elif action == 2 or action == 5:  \n",
    "                            fallback_action = 0\n",
    "                            node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "\n",
    "                    # Compute reward\n",
    "                    reward = get_reward(chosen_answer[0], gold_answer, action, num_llm_calls, alpha, beta)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    # if normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                    if are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    next_state = get_state(node)\n",
    "\n",
    "                    if not fallback_used:\n",
    "                        agent.replay_buffer.append((state, action, reward, next_state, False))  \n",
    "                    else:\n",
    "                        agent.replay_buffer.append((state, fallback_action, reward, next_state, False))\n",
    "\n",
    "                    if action == 2 or action == 5:\n",
    "                        num_children = len(node.get(\"sons\", []))\n",
    "                        child_reward = reward / num_children  \n",
    "                        for child_state, child_action, _, next_child_state, done in child_experiences:\n",
    "                            agent.replay_buffer.append((child_state, child_action, reward, next_child_state, done))\n",
    "\n",
    "                    # update total number of llm calls in this episode\n",
    "                    total_num_llm_calls += num_llm_calls\n",
    "\n",
    "                    agent.train(batch_size)\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "        rewards_list.append(total_reward)\n",
    "        accuracy_list.append(accuracy)\n",
    "        total_num_llm_calls_list.append(total_num_llm_calls)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_agent = copy.deepcopy(agent)\n",
    "\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Total LLM Calls in this episode: {total_num_llm_calls}\")\n",
    "        print(f\"Epsilon: {epsilon:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Store the rewards and accuracy for this configuration\n",
    "    accuracy_rewards_numcalls[config_name] = {\n",
    "        \"rewards\": rewards_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"num_calls\": total_num_llm_calls_list\n",
    "    }\n",
    "\n",
    "    # Save model after training with each configuration\n",
    "    model_path = f\"saved_models_with_reformulation_trained_all/agent_{config_name}.pth\"\n",
    "    torch.save(agent.q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the best model overallx\n",
    "    if best_agent is not None:\n",
    "        best_model_path = f\"saved_models_with_reformulation_trained_all/best_agent_{config_name}.pth\"\n",
    "        torch.save(best_agent.q_network.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dynamically extract data for each configuration\n",
    "config_names = list(accuracy_rewards_numcalls.keys())\n",
    "normalized_rewards = {}\n",
    "accuracies = {}\n",
    "num_calls = {}\n",
    "\n",
    "for config_name in config_names:\n",
    "    rewards = np.array(accuracy_rewards_numcalls[config_name][\"rewards\"])\n",
    "    accuracy = accuracy_rewards_numcalls[config_name][\"accuracy\"]\n",
    "    calls = accuracy_rewards_numcalls[config_name][\"num_calls\"]\n",
    "\n",
    "    # Normalize rewards\n",
    "    normalized_rewards[config_name] = (rewards - np.min(rewards)) / (np.max(rewards) - np.min(rewards))\n",
    "    accuracies[config_name] = accuracy\n",
    "    num_calls[config_name] = calls\n",
    "\n",
    "# Plotting normalized rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for config_name in config_names:\n",
    "    plt.plot(normalized_rewards[config_name], label=config_name)\n",
    "plt.title(\"Normalized Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Normalized Reward\")\n",
    "plt.legend()\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for config_name in config_names:\n",
    "    plt.plot(accuracies[config_name], label=config_name)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting number of LLM calls\n",
    "plt.figure(figsize=(8, 6))\n",
    "for config_name in config_names:\n",
    "    plt.plot(num_calls[config_name], label=config_name)\n",
    "plt.title(\"Number of LLM Calls\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Calls\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of calls for each configuration\n",
    "config_names = list(accuracy_rewards_numcalls.keys())\n",
    "for config in config_names:\n",
    "    num_calls = accuracy_rewards_numcalls[config][\"num_calls\"][-1]\n",
    "    print(\"config\", config,  \"num calls\", num_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent on the test data\n",
    "def evaluate_agent(agent, data, q2gold):\n",
    "    agent.q_network.eval()  # Set the model to evaluation mode\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                total_parent_nodes += 1\n",
    "                state = get_state(node)\n",
    "                action = agent.select_action(state, epsilon=0)  # No exploration during evaluation\n",
    "                chosen_answer = None\n",
    "                fallback_used = False  # Track if a fallback was used\n",
    "                fallback_action = None\n",
    "\n",
    "                question, _ = get_question_after_resolving_references(node, example)\n",
    "                # Simulate the action\n",
    "                if action == 0:\n",
    "                    node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                elif action == 1:  \n",
    "                    node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                elif action == 2:  \n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "\n",
    "                    for node_ in tree_with_answers_chosen_by_agent:\n",
    "                        question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "                    \n",
    "                        if len(node_[\"sons\"]) == 0:\n",
    "                            child_state = get_state(node_, depth=1)\n",
    "                            child_action = agent.select_action(child_state, epsilon)\n",
    "                            if child_action == 0:  \n",
    "                                node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 1  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                            elif child_action == 1:  \n",
    "                                node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 0  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                            elif child_action == 3:\n",
    "                                # CB with reformulation\n",
    "                                reformulated_question = reformulate_question(question_)\n",
    "                                node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 4\n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                            elif child_action == 4:\n",
    "                                # OB with reformulation\n",
    "                                reformulated_question = reformulate_question(question_)\n",
    "                                node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 3\n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                            else:\n",
    "                                # Throw exception\n",
    "                                raise ValueError(\"Invalid action selected for child node : \" + action_space[child_action])\n",
    "\n",
    "                            node_[\"answer\"] = child_answer\n",
    "                        else:\n",
    "                            # If the node has children, aggregate the answers\n",
    "                            node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                            chosen_answer = node_[\"child_answer\"]\n",
    "                    \n",
    "                    # TODO i am not counting fallbacks here in the children\n",
    "                elif action == 3:\n",
    "                    # CB with reformulation\n",
    "                    reformulated_question = reformulate_question(question)\n",
    "                    chosen_answer = get_cb_answer(reformulated_question, dataset_used)\n",
    "                \n",
    "                elif action == 4:\n",
    "                    # OB with reformulation\n",
    "                    reformulated_question = reformulate_question(question)\n",
    "                    chosen_answer = get_singlehop_ob_answer(reformulated_question, [], dataset_used)\n",
    "                elif action == 5:\n",
    "                    # Child with reformulation\n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "\n",
    "                    for node_ in tree_with_answers_chosen_by_agent:\n",
    "                        question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "\n",
    "                        if len(node_[\"sons\"]) == 0:\n",
    "                            child_state = get_state(node_, depth=1)\n",
    "                            child_action = agent.select_action(child_state, epsilon)\n",
    "                            if child_action == 0:  \n",
    "                                node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 1  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                            elif child_action == 1:  \n",
    "                                node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 0  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                            elif child_action == 3:\n",
    "                                # CB with reformulation\n",
    "                                reformulated_question = reformulate_question(question_)\n",
    "                                node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 4\n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                            elif child_action == 4:\n",
    "                                # OB with reformulation\n",
    "                                reformulated_question = reformulate_question(question_)\n",
    "                                node_[\"ob_answer\"] = get_singlehop_ob_answer(reformulated_question, topic_entities, dataset_used)\n",
    "                                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                    child_action = 3\n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(reformulated_question, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                            else:\n",
    "                                # throw error\n",
    "                                raise Exception('Wrong action decided for the children ?? ') # Don't! If you catch, likely to hide bugs.\n",
    "\n",
    "                            node_[\"answer\"] = child_answer\n",
    "                        else:\n",
    "                            node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                            chosen_answer = node_[\"child_answer\"]\n",
    "                    \n",
    "\n",
    "                if \"unknown\" in chosen_answer[0].lower().strip():\n",
    "                    fallback_used = True\n",
    "                    if action == 0 or action == 3:\n",
    "                        fallback_action = 1\n",
    "                        node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                    elif action == 1 or action == 4:\n",
    "                        fallback_action = 0\n",
    "                        node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                    elif action == 2 or action == 5:  \n",
    "                        fallback_action = 0\n",
    "                        node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "\n",
    "                # Compute reward\n",
    "                # if chosen_answer and normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": chosen_answer[0],\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    \n",
    "    epsilon = config[\"epsilon\"]\n",
    "    epsilon_min = config[\"epsilon_min\"]\n",
    "    epsilon_decay = config[\"epsilon_decay\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_episodes = config[\"num_episodes\"]\n",
    "    lr = config[\"lr\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "\n",
    "    config_name = config[\"name\"]\n",
    "    # model_path = f\"saved_models_with_reformulation_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    model_path = f\"saved_models_with_reformulation_trained_all/agent_{config_name}.pth\"\n",
    "\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}, hidden_dim={hidden_dim}, lr={lr}, gamma={gamma}\")\n",
    "    # Load the trained model\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    \n",
    "    epsilon = config[\"epsilon\"]\n",
    "    epsilon_min = config[\"epsilon_min\"]\n",
    "    epsilon_decay = config[\"epsilon_decay\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_episodes = config[\"num_episodes\"]\n",
    "    lr = config[\"lr\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "\n",
    "    config_name = config[\"name\"]\n",
    "    # model_path = f\"saved_models_with_reformulation_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    model_path = f\"saved_models_with_reformulation_trained_all/agent_{config_name}.pth\"\n",
    "\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}, hidden_dim={hidden_dim}, lr={lr}, gamma={gamma}\")\n",
    "    # Load the trained model\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    \n",
    "    epsilon = config[\"epsilon\"]\n",
    "    epsilon_min = config[\"epsilon_min\"]\n",
    "    epsilon_decay = config[\"epsilon_decay\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_episodes = config[\"num_episodes\"]\n",
    "    lr = config[\"lr\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "\n",
    "    config_name = config[\"name\"]\n",
    "    # model_path = f\"saved_models_with_reformulation_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    model_path = f\"saved_models_with_reformulation_trained_all/agent_{config_name}.pth\"\n",
    "\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}, hidden_dim={hidden_dim}, lr={lr}, gamma={gamma}\")\n",
    "    # Load the trained model\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach with deep learning (Transformers networks) using question only in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, nhead=4, num_transformer_layers=2):\n",
    "        super(TransformerQNetwork, self).__init__()\n",
    "        \n",
    "        # Input embedding (maps state_dim to hidden_dim)\n",
    "        self.embedding_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim * 4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "        \n",
    "        # Output layer (maps Transformer output to action_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "          state -> Shape: [sequence_length, batch_size, state_dim]\n",
    "        Output:\n",
    "          Q-values for each action -> Shape: [batch_size, action_dim]\n",
    "        \"\"\"\n",
    "        # Embed input state\n",
    "        embedded_state = self.embedding_layer(state)  # Shape: [sequence_length, batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(embedded_state)  # Shape: [sequence_length, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use only the first sequence element (sequence_length=1) and map to action_dim\n",
    "        q_values = self.output_layer(transformer_output.squeeze(0))  # Shape: [batch_size, action_dim]\n",
    "        return q_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "# # Define the reward function\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     if normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def get_reward(chosen_answer, gold_answer):\n",
    "#     # Compute embeddings for the chosen and gold answers\n",
    "#     chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "#     gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "#     # Compute cosine similarity\n",
    "#     similarity = util.cos_sim(chosen_embedding, gold_embedding).item()\n",
    "#     return similarity  # Reward is the similarity score (between -1 and 1)\n",
    "\n",
    "def get_reward(chosen_answer, gold_answer, action, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "    \"\"\"\n",
    "    Reward function with tradeoff between accuracy and efficiency.\n",
    "    :param chosen_answer: Answer chosen by the agent.\n",
    "    :param gold_answer: Ground truth answer.\n",
    "    :param action: The action selected by the agent (0=CB, 1=OB, 2=Child).\n",
    "    :param num_llm_calls: The number of LLM calls made during the current decision.\n",
    "    :param alpha: Weight for accuracy reward.\n",
    "    :param beta: Weight for efficiency penalty.\n",
    "    :return: A reward value that balances accuracy and efficiency.\n",
    "    \"\"\"\n",
    "    # Compute Accuracy Reward\n",
    "    chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "    gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(chosen_embedding, gold_embedding).item()  # Value between -1 (opposite) and 1 (exact match)\n",
    "    accuracy_reward = max(0, similarity)  # Ensure rewards are non-negative\n",
    "\n",
    "    # Define LLM cost for each action\n",
    "    ACTION_COSTS = {\n",
    "        0: 1,  # CB cost\n",
    "        1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "        2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "    }\n",
    "\n",
    "    # Compute Efficiency Penalty\n",
    "    action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "    efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "    # Combine Accuracy and Efficiency\n",
    "    reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    # child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    # child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 3 = 1152\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    # child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 2)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 2)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    # child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        # cb_logprob +  # CB log probabilities\n",
    "        # ob_logprob +  # OB log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        list(question_embedding) +  # Semantic embedding of the question\n",
    "        # list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        # list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        [cb_confidence, ob_confidence] +  # Confidence scores for CB and OB\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length]  # Answer quality features\n",
    "    )\n",
    "\n",
    "    # maybe try question only by masking all and add log probs for the solved one and try adding verification step in the end.\n",
    "\n",
    "    return torch.FloatTensor(state)\n",
    "\n",
    "HAS_CHILDREN_INDEX = 0\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.q_network = TransformerQNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = TransformerQNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        # Add sequence and batch dimensions for a single state, required by Transformer\n",
    "        state = state.unsqueeze(0).unsqueeze(0)  # Shape becomes [1 (sequence_length), 1 (batch_size), state_dim]\n",
    "\n",
    "        if random.random() < epsilon:  # Exploration (random action)\n",
    "            # Check if the current node has children; restrict actions accordingly\n",
    "            if state[0, 0, HAS_CHILDREN_INDEX] == 0:  # No children, Child action is invalid\n",
    "                return random.randint(0, len(action_space) - 2)  # Choose CB or OB\n",
    "            else:\n",
    "                return random.randint(0, len(action_space) - 1)  # Choose CB, OB, or Child\n",
    "        else:  # Exploitation (choose best action using Q-network)\n",
    "            with torch.no_grad():\n",
    "                # Forward pass through Q-network to get Q-values for available actions\n",
    "                q_values = self.q_network(state).squeeze(0).squeeze(0)  # Output shape: [action_dim]\n",
    "                \n",
    "                # Mask invalid actions (disable \"Child\" action if node has no children)\n",
    "                if state[0, 0, HAS_CHILDREN_INDEX] == 0:  # No children\n",
    "                    q_values[-1] = -float('inf')  # Mask out \"Child\" action\n",
    "\n",
    "                # Select action with the highest Q-value\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a mini-batch of transitions\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert lists to PyTorch Tensors\n",
    "        states = torch.stack(states)  # Shape: [batch_size, state_dim]\n",
    "        actions = torch.LongTensor(actions)  # Shape: [batch_size]\n",
    "        rewards = torch.FloatTensor(rewards)  # Shape: [batch_size]\n",
    "        next_states = torch.stack(next_states)  # Shape: [batch_size, state_dim]\n",
    "        dones = torch.FloatTensor(dones)  # Shape: [batch_size]\n",
    "\n",
    "        # Add sequence length dimension (sequence_length=1) for Transformer input\n",
    "        states = states.unsqueeze(0)  # Shape: [sequence_length=1, batch_size, state_dim]\n",
    "        next_states = next_states.unsqueeze(0)  # Shape: [sequence_length=1, batch_size, state_dim]\n",
    "\n",
    "        # Compute Q-values for current states using the QNetwork\n",
    "        q_values = self.q_network(states)  # Shape: [sequence_length=1, batch_size, action_dim]\n",
    "        q_values = q_values.squeeze(0)  # Remove sequence dimension: [batch_size, action_dim]\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1))  # Q-values of taken actions: [batch_size, 1]\n",
    "\n",
    "        # Compute target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)  # Shape: [sequence_length=1, batch_size, action_dim]\n",
    "            next_q_values = next_q_values.squeeze(0)  # Remove sequence dimension: [batch_size, action_dim]\n",
    "            max_next_q_values = next_q_values.max(1)[0]  # Max Q-values for each batch: [batch_size]\n",
    "\n",
    "            # Bellman equation for target Q-values\n",
    "            target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)  # Shape: [batch_size]\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        # loss = nn.MSELoss()(q_values.squeeze(), target_q_values)  # q_values.squeeze(): [batch_size], target_q_values: [batch_size]\n",
    "        loss = nn.SmoothL1Loss()(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs(\"saved_models_transformers_question_only_state_trained_all\", exist_ok=True)\n",
    "\n",
    "# Define different configurations for alpha and beta\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "accuracy_rewards_numcalls = {}\n",
    "\n",
    "# Loop through different settings\n",
    "for config in configurations:\n",
    "    alpha = config[\"alpha\"]\n",
    "    beta = config[\"beta\"]\n",
    "    config_name = config[\"name\"]\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    action_space = ['CB', 'OB', 'Child']  # Possible actions\n",
    "    state_dim = 7 + 384 + 2 + 2 + 2  # 100 + 7 + 1152 + 2 + 2 + 2 # Number of features in the state vector\n",
    "    action_dim = len(action_space)  # Number of actions (CB, OB, Child)\n",
    "    hidden_dim = 128  # Hidden layer size\n",
    "    lr = 5e-3  # Increase from 1e-3  //     1e-3  # Learning rate\n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    epsilon_decay = 0.98  # Decay rate for exploration\n",
    "    batch_size = 32  # Mini-batch size\n",
    "    num_episodes = 50  # Number of training episodes\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "\n",
    "    print(f\"\\nTraining with {config_name} (alpha={alpha}, beta={beta})\")\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_agent = None\n",
    "\n",
    "    # Store rewards, accuracy and total number of LLM calls for each episode\n",
    "    rewards_list = []\n",
    "    accuracy_list = []\n",
    "    total_num_llm_calls_list = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    agent.q_network.train()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        total_reward = 0\n",
    "        correct_answers = 0\n",
    "        total_parent_nodes = 0\n",
    "        total_num_llm_calls = 0\n",
    "\n",
    "        for example in data:\n",
    "            for node in example:\n",
    "                if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                    gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                    total_parent_nodes += 1\n",
    "                    state = get_state(node)  \n",
    "                    action = agent.select_action(state, epsilon)  \n",
    "                    chosen_answer = None\n",
    "                    fallback_used = False  \n",
    "                    num_llm_calls = None\n",
    "\n",
    "                    question, _ = get_question_after_resolving_references(node, example)\n",
    "                    # Simulate the action\n",
    "                    if action == 0:  \n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 1:  \n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 2:  \n",
    "                        tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                        child_experiences = []\n",
    "\n",
    "                        for node_ in tree_with_answers_chosen_by_agent:\n",
    "                            question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "                        \n",
    "                            if len(node_[\"sons\"]) == 0:\n",
    "                                child_state = get_state(node_, depth=1)\n",
    "                                child_action = agent.select_action(child_state, epsilon)\n",
    "                                if child_action == 0:  \n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 1  \n",
    "                                        node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                        child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                elif child_action == 1:  \n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                    if \"unknown\" in child_answer[0].lower().strip():\n",
    "                                        child_action = 0  \n",
    "                                        node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                        child_answer = node_.get(\"cb_answer\", [None])\n",
    "\n",
    "                                node_[\"answer\"] = child_answer\n",
    "                                child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "                            else:\n",
    "                                node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                                chosen_answer = node_[\"child_answer\"]\n",
    "                        \n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation)\n",
    "\n",
    "                    if \"unknown\" in chosen_answer[0].lower().strip():\n",
    "                        fallback_used = True\n",
    "                        if action == 0:  \n",
    "                            fallback_action = 1\n",
    "                            node['ob_answer'] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        elif action == 1:  \n",
    "                            fallback_action = 0\n",
    "                            node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        elif action == 2:  \n",
    "                            fallback_action = 0\n",
    "                            node['cb_answer'] = get_cb_answer(question, dataset_used)\n",
    "                            chosen_answer = node.get(\"cb_answer\", [None])\n",
    "\n",
    "                    # Compute reward\n",
    "                    reward = get_reward(chosen_answer[0], gold_answer, action, num_llm_calls, alpha, beta)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    # if normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                    if are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    next_state = get_state(node)\n",
    "\n",
    "                    if not fallback_used:\n",
    "                        agent.replay_buffer.append((state, action, reward, next_state, False))  \n",
    "                    else:\n",
    "                        agent.replay_buffer.append((state, fallback_action, reward, next_state, False))\n",
    "\n",
    "                    if action == 2:\n",
    "                        num_children = len(node.get(\"sons\", []))\n",
    "                        child_reward = reward / num_children  \n",
    "                        for child_state, child_action, _, next_child_state, done in child_experiences:\n",
    "                            agent.replay_buffer.append((child_state, child_action, reward, next_child_state, done))\n",
    "\n",
    "                    # update total number of llm calls in this episode\n",
    "                    total_num_llm_calls += num_llm_calls\n",
    "\n",
    "                    agent.train(batch_size)\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "        rewards_list.append(total_reward)\n",
    "        accuracy_list.append(accuracy)\n",
    "        total_num_llm_calls_list.append(total_num_llm_calls)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_agent = copy.deepcopy(agent)\n",
    "\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Total LLM Calls in this episode: {total_num_llm_calls}\")\n",
    "        print(f\"Epsilon: {epsilon:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Store the rewards and accuracy for this configuration\n",
    "    accuracy_rewards_numcalls[config_name] = {\n",
    "        \"rewards\": rewards_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"num_calls\": total_num_llm_calls_list\n",
    "    }\n",
    "\n",
    "    # Save model after training with each configuration\n",
    "    model_path = f\"saved_models_transformers_question_only_state_trained_all/agent_{config_name}.pth\"\n",
    "    torch.save(agent.q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the best model overall\n",
    "    if best_agent is not None:\n",
    "        best_model_path = f\"saved_models_transformers_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "        torch.save(best_agent.q_network.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get rewards, accuracy and number of LLM calls for each configuration\n",
    "balanced_rewards = accuracy_rewards_numcalls[\"Balanced\"][\"rewards\"]\n",
    "balanced_accuracy = accuracy_rewards_numcalls[\"Balanced\"][\"accuracy\"]\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"]\n",
    "\n",
    "efficiency_rewards = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"rewards\"]\n",
    "efficiency_accuracy = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"accuracy\"]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"]\n",
    "\n",
    "high_accuracy_accuracy = accuracy_rewards_numcalls[\"High_Accuracy\"][\"accuracy\"]\n",
    "high_accuracy_rewards = accuracy_rewards_numcalls[\"High_Accuracy\"][\"rewards\"]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"]\n",
    "\n",
    "# min max scaling for rewards\n",
    "balanced_rewards = (balanced_rewards - np.min(balanced_rewards)) / (np.max(balanced_rewards) - np.min(balanced_rewards))\n",
    "efficiency_rewards = (efficiency_rewards - np.min(efficiency_rewards)) / (np.max(efficiency_rewards) - np.min(efficiency_rewards))\n",
    "high_accuracy_rewards = (high_accuracy_rewards - np.min(high_accuracy_rewards)) / (np.max(high_accuracy_rewards) - np.min(high_accuracy_rewards))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(balanced_rewards, label=\"Balanced\")\n",
    "plt.plot(efficiency_rewards, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_rewards, label=\"High Accuracy\")\n",
    "plt.title(\"Normalized Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Normalized Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(balanced_accuracy, label=\"Balanced\")\n",
    "plt.plot(efficiency_accuracy, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_accuracy, label=\"High Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(balanced_num_calls, label=\"Balanced\")\n",
    "plt.plot(efficiency_num_calls, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_num_calls, label=\"High Accuracy\")\n",
    "plt.title(\"Number of LLM Calls\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Calls\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of calls for each configuration\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"][-1]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"][-1]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"][-1]\n",
    "\n",
    "print(\"balanced_num_calls\", balanced_num_calls)\n",
    "print(\"efficiency_num_calls\", efficiency_num_calls)\n",
    "print(\"high_accuracy_num_calls\", high_accuracy_num_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent on the test data\n",
    "def evaluate_agent(agent, data, q2gold):\n",
    "    agent.q_network.eval()  # Set the model to evaluation mode\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                total_parent_nodes += 1\n",
    "                state = get_state(node)\n",
    "                action = agent.select_action(state, epsilon=0)  # No exploration during evaluation\n",
    "                chosen_answer = None\n",
    "                fallback_used = False  # Track if a fallback was used\n",
    "                fallback_action = None\n",
    "\n",
    "                question, _ = get_question_after_resolving_references(node, example)\n",
    "                # Simulate the action (choose answer based on action)\n",
    "                if action == 0:  # CB\n",
    "                    node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to OB\n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                        fallback_action = 1\n",
    "                        fallback_used = True\n",
    "                elif action == 1:  # OB\n",
    "                    node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                        fallback_action = 0\n",
    "                        fallback_used = True\n",
    "                elif action == 2:  # Child\n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                    for child_idx in node.get(\"sons\", []):\n",
    "                        child = tree_with_answers_chosen_by_agent[child_idx]\n",
    "                        child_state = get_state(child, depth=1)\n",
    "                        child_action = agent.select_action(child_state, epsilon=0)\n",
    "                        question_, topic_entities = get_question_after_resolving_references(child, tree_with_answers_chosen_by_agent)\n",
    "                        if child_action == 0:  # CB\n",
    "                            child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                            child_answer = child.get(\"cb_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to OB\n",
    "                                child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = child.get(\"ob_answer\", [None])\n",
    "                        elif child_action == 1:  # OB\n",
    "                            child[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                            child_answer = child.get(\"ob_answer\", [None])\n",
    "                            if \"unknown\" in child_answer[0].lower().strip():  # Fallback to CB\n",
    "                                child[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = child.get(\"cb_answer\", [None])\n",
    "\n",
    "                        tree_with_answers_chosen_by_agent[child_idx][\"answer\"] = child_answer\n",
    "\n",
    "                    # Generate child_answer for the parent node\n",
    "                    # print(\"tree_with_answers_chosen_by_agent\", tree_with_answers_chosen_by_agent)\n",
    "                    node[\"child_answer\"], node[\"answer\"] = aggregate_multihop_answer(node, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                    chosen_answer = node[\"child_answer\"][0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB or OB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]  # Try CB first\n",
    "                        # fallback_used = True\n",
    "                        if \"unknown\" in chosen_answer.lower().strip():\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])[0]  # Try OB next\n",
    "\n",
    "                # Compute reward\n",
    "                # if chosen_answer and normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(chosen_answer, gold_answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": chosen_answer,\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "\n",
    "# List of trained models to evaluate\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"}\n",
    "]\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_transformers_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_transformers_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_transformers_question_only_state_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(agent, test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_action(is_child):\n",
    "    if is_child == True:\n",
    "        return random.randint(0, len(action_space) - 2)\n",
    "    else:\n",
    "        return random.randint(0, len(action_space) - 1)\n",
    "            \n",
    "# Evaluate the agent on the test data\n",
    "def evaluate_random_agent(data, q2gold):\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"].strip()]\n",
    "                total_parent_nodes += 1\n",
    "                if len(node.get(\"sons\", [])) != 0:  # Leaf node\n",
    "                    action = select_random_action(is_child=False)\n",
    "                else:\n",
    "                    action = select_random_action(is_child=True)\n",
    "                chosen_answer = None\n",
    "                fallback_used = False  # Track if a fallback was used\n",
    "                fallback_action = None\n",
    "\n",
    "                question, _ = get_question_after_resolving_references(node, example)\n",
    "                # Simulate the action (choose answer based on action)\n",
    "                if action == 0:  # CB\n",
    "                    node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to OB\n",
    "                        node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                        fallback_action = 1\n",
    "                        fallback_used = True\n",
    "                elif action == 1:  # OB\n",
    "                    node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])[0]\n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]\n",
    "                        fallback_action = 0\n",
    "                        fallback_used = True\n",
    "                elif action == 2:  # Child\n",
    "                    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "                    for node_ in tree_with_answers_chosen_by_agent:  # Process each node in the copied tree\n",
    "                        question_, topic_entities = get_question_after_resolving_references(node_, tree_with_answers_chosen_by_agent)\n",
    "\n",
    "                        # Leaf Node -> Execute Child-Specific Actions\n",
    "                        if len(node_[\"sons\"]) == 0:\n",
    "                            child_action = select_random_action(is_child=True)\n",
    "                            if child_action == 0:  # CB\n",
    "                                node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():  # Fallback to OB\n",
    "                                    child_action = 1\n",
    "                                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "                            elif child_action == 1:  # OB\n",
    "                                node_[\"ob_answer\"] = get_singlehop_ob_answer(question_, topic_entities, dataset_used)\n",
    "                                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                                if \"unknown\" in child_answer[0].lower().strip():  # Fallback to CB\n",
    "                                    child_action = 0\n",
    "                                    node_[\"cb_answer\"] = get_cb_answer(question_, dataset_used)\n",
    "                                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "                        else:\n",
    "                            node[\"child_answer\"], node[\"answer\"] = aggregate_multihop_answer(node, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "                            chosen_answer = node[\"child_answer\"][0]\n",
    "                    \n",
    "                    if \"unknown\" in chosen_answer.lower().strip():  # Fallback to CB or OB\n",
    "                        node[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])[0]  # Try CB first\n",
    "                        if \"unknown\" in chosen_answer.lower().strip():\n",
    "                            fallback_action = 1\n",
    "                            fallback_used = True\n",
    "                            node[\"ob_answer\"] = get_singlehop_ob_answer(question, [], dataset_used)\n",
    "                            chosen_answer = node.get(\"ob_answer\", [None])[0]  # Try OB next\n",
    "\n",
    "                # Compute reward\n",
    "                # if chosen_answer and normalize_answer(chosen_answer) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(chosen_answer, gold_answer):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": node[\"question_text\"],\n",
    "                    \"answer\": chosen_answer,\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "# List of trained models to evaluate\n",
    "configurations = [\n",
    "    {\"name\": \"Random\"},\n",
    "]\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_random_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_random_agent(test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_random_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_random_agent(test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    model_path = f\"saved_models_random_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_random_agent(test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approach with deep learning (using Question + cb + ob + RESAMPLING ACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data and question decompositions\n",
    "raw_data = [json.loads(line.strip()) for line in open('./hotpotqa__v2_dev_random_100.jsonl')]\n",
    "raw_data_2wiki = [json.loads(line.strip()) for line in open('./2wikimultihopqa__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_2wiki:\n",
    "    item['dataset'] = '2wiki'\n",
    "raw_data.extend(raw_data_2wiki)\n",
    "raw_data_musique = [json.loads(line.strip()) for line in open('./musique_ans__v2_dev_random_100.jsonl')]\n",
    "for item in raw_data_musique:\n",
    "    item['dataset'] = 'musique'\n",
    "raw_data.extend(raw_data_musique)\n",
    "\n",
    "q2dq = json.load(open(\"./question_decompositions-devset-hotpotqa.json\"))\n",
    "q2dq_2wiki = json.load(open(\"./question_decompositions-devset-2wiki.json\"))\n",
    "q2dq_musique = json.load(open(\"./question_decompositions-devset-musique.json\"))\n",
    "q2dq.update(q2dq_2wiki)\n",
    "q2dq.update(q2dq_musique)\n",
    "\n",
    "# Create q2gold map\n",
    "q2gold = {}\n",
    "for item in raw_data:\n",
    "    try:\n",
    "        question = item['question_text'].strip()\n",
    "        question = list(q2dq[question].keys())[0]\n",
    "        gold = item['answers_objects'][0]['spans'][0]\n",
    "        q_type = item['dataset']\n",
    "        q2gold[question] = (gold, q_type)\n",
    "    except Exception as e:\n",
    "        # Skip if question not found in question_decompositions\n",
    "        continue\n",
    "\n",
    "# Load the data to analyze\n",
    "with open('results-devset-hotpotqa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('results-devset-2wiki.json', 'r') as file:\n",
    "    data_2wiki = json.load(file)\n",
    "with open('results-devset-musique.json', 'r') as file:\n",
    "    data_musique = json.load(file)\n",
    "\n",
    "data.extend(data_2wiki)\n",
    "data.extend(data_musique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ['CB', 'OB', 'Child', 'ResampleTreeDecompositionThenSolve']  # Possible actions\n",
    "success_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0, \"resampletreedecompositionthensolve\": 0}\n",
    "attempt_counts = {\"cb\": 0, \"ob\": 0, \"child\": 0, \"resampletreedecompositionthensolve\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "\n",
    "def pad_or_truncate(logprobs, max_length=50, pad_value=-100):\n",
    "    if len(logprobs) < max_length:\n",
    "        # Pad with pad_value\n",
    "        return logprobs + [pad_value] * (max_length - len(logprobs))\n",
    "    else:\n",
    "        # Truncate to max_length\n",
    "        return logprobs[:max_length]\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# def get_reward(chosen_answer, gold_answer, action, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "#     \"\"\"\n",
    "#     Reward function with tradeoff between accuracy and efficiency.\n",
    "#     :param chosen_answer: Answer chosen by the agent.\n",
    "#     :param gold_answer: Ground truth answer.\n",
    "#     :param action: The action selected by the agent (0=CB, 1=OB, 2=Child).\n",
    "#     :param num_llm_calls: The number of LLM calls made during the current decision.\n",
    "#     :param alpha: Weight for accuracy reward.\n",
    "#     :param beta: Weight for efficiency penalty.\n",
    "#     :return: A reward value that balances accuracy and efficiency.\n",
    "#     \"\"\"\n",
    "#     # Compute Accuracy Reward\n",
    "#     chosen_embedding = model.encode(chosen_answer, convert_to_tensor=True)\n",
    "#     gold_embedding = model.encode(gold_answer, convert_to_tensor=True)\n",
    "#     similarity = util.cos_sim(chosen_embedding, gold_embedding).item()  # Value between -1 (opposite) and 1 (exact match)\n",
    "#     accuracy_reward = (similarity + 1) / 2  # Normalize to range [0, 1]\n",
    "\n",
    "#     # Define LLM cost for each action\n",
    "#     ACTION_COSTS = {\n",
    "#         0: 1,  # CB cost\n",
    "#         1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "#         2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "#         3: 1,  # Tree Resampling cost\n",
    "#     }\n",
    "\n",
    "#     # Compute Efficiency Penalty\n",
    "#     action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "#     efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "#     print(f\"Action: {action}, Chosen Answer: {chosen_answer}, Gold Answer: {gold_answer}, accuracy_reward: {accuracy_reward:.2f}, Efficiency Penalty: {efficiency_penalty}\")\n",
    "#     # Combine Accuracy and Efficiency\n",
    "#     reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "#     return reward\n",
    "\n",
    "# use llm to get the reward\n",
    "def get_reward(action, accuracy_reward, num_llm_calls, alpha=1.0, beta=0.1):\n",
    "\n",
    "    # Define LLM cost for each action\n",
    "    ACTION_COSTS = {\n",
    "        0: 1,  # CB cost\n",
    "        1: 1,  # OB cost (higher than CB because of retrieval)\n",
    "        2: 1,  # Child decomposition cost (higher due to multiple child evaluations)\n",
    "        3: 1,  # Tree Resampling cost\n",
    "    }\n",
    "\n",
    "    # Compute Efficiency Penalty\n",
    "    action_cost = ACTION_COSTS.get(action, 1)  # Default to 1 if action is unrecognized\n",
    "    efficiency_penalty = num_llm_calls * action_cost  # Penalize based on LLM usage\n",
    "\n",
    "    # print(f\"Action: {action}, accuracy_reward: {accuracy_reward:.2f}, Efficiency Penalty: {efficiency_penalty}\")\n",
    "    # Combine Accuracy and Efficiency\n",
    "    reward = alpha * accuracy_reward - beta * efficiency_penalty\n",
    "\n",
    "    return reward\n",
    "\n",
    "def get_state(node, depth=0, max_length=50):\n",
    "    # Extract raw features\n",
    "    cb_logprob = node.get(\"cb_answer\", [None, None, None, []])[3] or []\n",
    "    ob_logprob = node.get(\"ob_answer\", [None, None, None, []])[3] or []\n",
    "    child_logprob = node.get(\"child_answer\", [None, None, None, []])[3] or []\n",
    "\n",
    "    # Pad or truncate logprobs each is of length 50\n",
    "    cb_logprob = pad_or_truncate(cb_logprob, max_length)\n",
    "    ob_logprob = pad_or_truncate(ob_logprob, max_length)\n",
    "    child_logprob = pad_or_truncate(child_logprob, max_length)\n",
    "\n",
    "    # Other features (7)\n",
    "    has_children = 1 if len(node.get(\"sons\", [])) else 0\n",
    "    question_length = len(node.get(\"question_text\", \"\").split())\n",
    "    question_type = encode_question_type(node.get(\"question_text\", \"\"))\n",
    "    num_children = len(node.get(\"sons\", []))\n",
    "    cb_success_rate = get_success_rate(\"cb\")\n",
    "    ob_success_rate = get_success_rate(\"ob\")\n",
    "    child_success_rate = get_success_rate(\"child\")\n",
    "\n",
    "    # Semantic features\n",
    "    # Each embedding is a 384-dimensional vector and they are total 4 = 384*4 = 1536\n",
    "    question_text = node.get(\"question_text\", \"\")\n",
    "    question_embedding = model.encode(question_text, convert_to_tensor=False)\n",
    "    cb_answer_embedding = model.encode(node.get(\"cb_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    ob_answer_embedding = model.encode(node.get(\"ob_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "    child_answer_embedding = model.encode(node.get(\"child_answer\", [\"\"])[0], convert_to_tensor=False)\n",
    "\n",
    "    # Confidence and uncertainty (total 3)\n",
    "    cb_confidence = node.get(\"cb_answer\", [None, None, None, []])[1] or 0.0\n",
    "    ob_confidence = node.get(\"ob_answer\", [None, None, None, []])[1] or 0.0\n",
    "    child_confidence = node.get(\"child_answer\", [None, None, None, []])[1] or 0.0\n",
    "\n",
    "    # Structural features (total 2)\n",
    "    tree_depth = depth\n",
    "    tree_position = 0 if depth == 0 else 1  # 0 for root, 1 for intermediate/leaf\n",
    "\n",
    "    # Temporal features (example: sliding window of last 3 actions) \n",
    "    # action_history = node.get(\"action_history\", [0, 0, 0])  # Placeholder for action history\n",
    "    # action_success_history = node.get(\"action_success_history\", [0, 0, 0])  # Placeholder for success history\n",
    "\n",
    "    # External knowledge features\n",
    "    # num_retrieved_documents = node.get(\"num_retrieved_documents\", 0)\n",
    "    # entity_linking_confidence = node.get(\"entity_linking_confidence\", 0.0)\n",
    "\n",
    "    # Answer quality (total 3)\n",
    "    cb_answer_length = len(node.get(\"cb_answer\", [\"\"])[0].split())\n",
    "    ob_answer_length = len(node.get(\"ob_answer\", [\"\"])[0].split())\n",
    "    child_answer_length = len(node.get(\"child_answer\", [\"\"])[0].split())\n",
    "\n",
    "    # Build state vector\n",
    "    state = (\n",
    "        cb_logprob +  # CB log probabilities\n",
    "        ob_logprob +  # OB log probabilities\n",
    "        child_logprob +  # Child log probabilities\n",
    "        # Discretized log probabilities\n",
    "        [has_children, question_length, question_type, num_children, cb_success_rate, ob_success_rate, child_success_rate] +  # Basic features\n",
    "        list(question_embedding) +  # Semantic embedding of the question\n",
    "        list(cb_answer_embedding) +  # Semantic embedding of the CB answer\n",
    "        list(ob_answer_embedding) +  # Semantic embedding of the OB answer\n",
    "        list(child_answer_embedding) +  # Semantic embedding of the Child answer\n",
    "        [cb_confidence, ob_confidence, child_confidence] +  # Confidence scores\n",
    "        [tree_depth, tree_position] +  # Structural features\n",
    "        [cb_answer_length, ob_answer_length, child_answer_length]  # Answer lengths\n",
    "    )\n",
    "\n",
    "    # maybe try question only by masking all and add log probs for the solved one and try adding verification step in the end.\n",
    "    return torch.FloatTensor(state)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            if state[150] == 0: # has children = 0, because child is not possible\n",
    "                return random.choice([0, 1]) # CB or OB\n",
    "            else:\n",
    "                return random.choice([0, 1, 2, 3]) # CB, OB or Child or resample\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                # print(q_values)\n",
    "                if state[150] == 0: # has children = 0\n",
    "                    q_values[-1] = -float('inf')\n",
    "                    q_values[-2] = -float('inf')\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # update the success rates\n",
    "        for i, action in enumerate(actions):\n",
    "            if rewards[i] == 1:\n",
    "                update_success_rate(action_space[action], True)\n",
    "            else:\n",
    "                update_success_rate(action_space[action], False)\n",
    "\n",
    "        # Compute loss and update the network\n",
    "        \n",
    "        # loss = nn.MSELoss()(q_values.squeeze(), target_q_values)\n",
    "        loss = nn.SmoothL1Loss()(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dim = 150 + 7 + 1536 + 3 + 2 + 3 # Number of features in the state vector\n",
    "# action_dim = len(action_space)  # Number of actions (CB, OB, Child)\n",
    "# hidden_dim = 128  # Hidden layer size\n",
    "# lr = 5e-3  # Increase from 1e-3  //     1e-3  # Learning rate\n",
    "# gamma = 0.99  # Discount factor\n",
    "# epsilon = 1.0  # Initial exploration rate\n",
    "# epsilon_min = 0.01  # Minimum exploration rate\n",
    "# epsilon_decay = 0.95  # Decay rate for exploration\n",
    "# batch_size = 128  # Mini-batch size\n",
    "# num_episodes = 30  # Number of training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def child_aggregation(example, dataset_used):\n",
    "    tree_with_answers_chosen_by_agent = copy.deepcopy(example)\n",
    "    child_experiences = []\n",
    "    chosen_answer = None\n",
    "    for node_ in tree_with_answers_chosen_by_agent:\n",
    "        question = node_[\"question_text\"].strip()\n",
    "        ref_tokens = re.findall(r\"<\\d+>\", question)\n",
    "        topic_entities = []\n",
    "        for ref_token in ref_tokens:\n",
    "            if \"fa\" in node_ and int(ref_token[1:-1]) <= len(tree_with_answers_chosen_by_agent[node_[\"fa\"]][\"sons\"]):\n",
    "                ref_idx = tree_with_answers_chosen_by_agent[node_[\"fa\"]][\"sons\"][int(ref_token[1:-1])-1]\n",
    "                if \"answer\" in tree_with_answers_chosen_by_agent[ref_idx]:\n",
    "                    question = question.replace(ref_token, tree_with_answers_chosen_by_agent[ref_idx][\"answer\"][0])\n",
    "                    topic_entities.append(tree_with_answers_chosen_by_agent[ref_idx][\"answer\"][0])\n",
    "        node_[\"question\"] = question\n",
    "\n",
    "        if len(node_[\"sons\"]) == 0:\n",
    "            child_state = get_state(node_, depth=1)\n",
    "            child_action = agent.select_action(child_state, epsilon)\n",
    "            if child_action == 0:  \n",
    "                node_[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                child_answer = node_.get(\"cb_answer\", [None])\n",
    "                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                    child_action = 1  \n",
    "                    node_[\"ob_answer\"] = get_singlehop_ob_answer(question, topic_entities, dataset_used)\n",
    "                    child_answer = node_.get(\"ob_answer\", [None])\n",
    "            elif child_action == 1:  \n",
    "                node_[\"ob_answer\"] = get_singlehop_ob_answer(question, topic_entities, dataset_used)\n",
    "                child_answer = node_.get(\"ob_answer\", [None])\n",
    "                if \"unknown\" in child_answer[0].lower().strip():\n",
    "                    child_action = 0  \n",
    "                    node_[\"cb_answer\"] = get_cb_answer(question, dataset_used)\n",
    "                    child_answer = node_.get(\"cb_answer\", [None])\n",
    "            else:\n",
    "                print(\"Invalid action for child node\", child_action)\n",
    "                raise ValueError(\"Invalid action for child node\")\n",
    "\n",
    "            node_[\"answer\"] = child_answer\n",
    "            child_experiences.append((child_state, child_action, 0, child_state, False))  \n",
    "        else:\n",
    "            node_[\"child_answer\"], node_[\"answer\"] = aggregate_multihop_answer(node_, tree_with_answers_chosen_by_agent, dataset_used)\n",
    "            chosen_answer = node_[\"child_answer\"]\n",
    "\n",
    "    return child_experiences, chosen_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs(\"saved_models_with_resampling_trained_all\", exist_ok=True)\n",
    "\n",
    "# Define different configurations for alpha and beta\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"},\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "accuracy_rewards_numcalls = {}\n",
    "\n",
    "# Loop through different settings\n",
    "for config in configurations:\n",
    "    alpha = config[\"alpha\"]\n",
    "    beta = config[\"beta\"]\n",
    "    config_name = config[\"name\"]\n",
    "    # initialize the parameters\n",
    "    state_dim = 150 + 7 + 1536 + 3 + 2 + 3 # Number of features in the state vector\n",
    "    action_dim = len(action_space)  # Number of actions (CB, OB, Child)\n",
    "    hidden_dim = 128  # Hidden layer size\n",
    "    lr = 5e-3  # Increase from 1e-3  //     1e-3  # Learning rate\n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    epsilon_decay = 0.95  # Decay rate for exploration\n",
    "    batch_size = 128  # Mini-batch size\n",
    "    num_episodes = 50  # Number of training episodes\n",
    "    \n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "\n",
    "    print(f\"\\nTraining with {config_name} (alpha={alpha}, beta={beta})\")\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_agent = None\n",
    "\n",
    "    # Store rewards, accuracy and total number of LLM calls for each episode\n",
    "    rewards_list = []\n",
    "    accuracy_list = []\n",
    "    total_num_llm_calls_list = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    agent.q_network.train()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "        total_reward = 0\n",
    "        correct_answers = 0\n",
    "        total_parent_nodes = 0\n",
    "        total_num_llm_calls = 0\n",
    "\n",
    "        for example in data:\n",
    "            for node in example:\n",
    "                if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                    gold_answer, dataset_used = q2gold[node[\"question_text\"]]\n",
    "                    total_parent_nodes += 1\n",
    "                    state = get_state(node)  \n",
    "                    action = agent.select_action(state, epsilon)\n",
    "                    # print(\"action selected \", action)\n",
    "                    done_resampling = False\n",
    "\n",
    "                    # saving original node question because maybe after resampling the question is changed\n",
    "                    original_node_question = node[\"question_text\"]\n",
    "                    if action == 3: # ResampleTreeDecompositionThenSolve\n",
    "                        # root_question = node[\"question_text\"].strip()\n",
    "                        root_question = node[\"question_text\"] # dont strip it, because we will search the original question\n",
    "                        # print(\"example before resampling\", example)\n",
    "                        example = tree_resampling_pipeline.resample_tree(root_question)\n",
    "                        # print(\"example after resampling\", example)\n",
    "                        # get the parent node again because maybe the tree is changed in term of the order\n",
    "                        for node_ in example:\n",
    "                            if \"fa\" not in node_:\n",
    "                                node = node_\n",
    "                                break\n",
    "                        done_resampling = True\n",
    "                        action = 2 # Set action now to Child to execute child aggregation\n",
    "                    chosen_answer = None\n",
    "                    fallback_used = False  \n",
    "                    num_llm_calls = None\n",
    "\n",
    "                    # Simulate the action\n",
    "                    if action == 0:\n",
    "                        node['cb_answer'] = get_cb_answer(node[\"question_text\"].strip(), dataset_used)\n",
    "                        chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        # chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 1:\n",
    "                        # since you are the parent, so topic entities are empty because no references\n",
    "                        node['ob_answer'] = get_singlehop_ob_answer(node[\"question_text\"].strip(), [], dataset_used)\n",
    "                        chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                        num_llm_calls = 1\n",
    "                    elif action == 2:\n",
    "                        # Call child aggregation function\n",
    "                        child_experiences, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                        num_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation)\n",
    "\n",
    "                    if done_resampling:\n",
    "                        # if the original action was resampling, then reset the action to 3\n",
    "                        # print(\"original action was resampling, so set action to 3\")\n",
    "                        action = 3\n",
    "                        num_llm_calls += 1 # because of the resampling\n",
    "\n",
    "                    # Get Q-values for the current state deep copy\n",
    "                    q_values = agent.q_network(state).clone()\n",
    "                    while (chosen_answer is None or \"unknown\" in chosen_answer[0].lower().strip()):\n",
    "                        fallback_used = True    \n",
    "\n",
    "                        # Mask the Q-value of the originally chosen action\n",
    "                        q_values[action] = -float('inf')  # Ignore the originally chosen action\n",
    "\n",
    "                        # if all q_values are -inf, then break\n",
    "                        if torch.all(q_values == -float('inf')):\n",
    "                            print(\"All actions are invalid, skipping fallback\")\n",
    "                            break\n",
    "\n",
    "                        # Mask invalid actions if applicable\n",
    "                        # For example, if state[150] == 0 (has_children == 0), disable actions 2 (Child) and 3 (Resample)\n",
    "                        if state[150] == 0:\n",
    "                            q_values[2] = -float('inf')  # Child aggregation\n",
    "                            q_values[3] = -float('inf')  # ResampleTreeDecomposition\n",
    "\n",
    "                        # Select the next best action\n",
    "                        fallback_action = torch.argmax(q_values).item()\n",
    "                        \n",
    "                        # set q_values of the fallback action to -inf in order to not select it again\n",
    "                        q_values[fallback_action] = -float('inf')  # Ignore the originally chosen action\n",
    "\n",
    "                        # Execute the fallback action\n",
    "                        if fallback_action == 0:  # CB\n",
    "                            chosen_answer = get_cb_answer(node[\"question_text\"].strip(), dataset_used)\n",
    "                            fallback_llm_calls = 1\n",
    "                        elif fallback_action == 1:  # OB\n",
    "                            chosen_answer = get_singlehop_ob_answer(node[\"question_text\"].strip(), [], dataset_used)\n",
    "                            fallback_llm_calls = 1\n",
    "                        elif fallback_action == 2:  # Child\n",
    "                            # Call your tree decomposition/resampling logic if required\n",
    "                            child_experiences, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                            fallback_llm_calls = len(child_experiences) + 1 + 1  # number of children + 1 (decomposition) + 1 (aggregation)\n",
    "                        elif fallback_action == 3:  # Resample\n",
    "                            # Call your tree resampling logic if required\n",
    "                            example = tree_resampling_pipeline.resample_tree(node[\"question_text\"].strip())\n",
    "                            child_experiences, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                            fallback_llm_calls = len(child_experiences) + 1 + 1 + 1 # number of children + 1 (decomposition) + 1 (aggregation) + 1 (resampling)\n",
    "                        \n",
    "                        # print(\"Fallback used, \", \"original action:\", action , \"fallback action:\", fallback_action, \"chosen answer:\", chosen_answer)\n",
    "\n",
    "                    # Compute reward\n",
    "                    # gold_answer, _ = q2gold[original_node_question]\n",
    "                    \n",
    "                    equivalent = are_answers_equivalent_using_llm(gold_answer, chosen_answer[0])\n",
    "                    if equivalent:\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    reward = get_reward(action, equivalent, num_llm_calls, alpha, beta)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    next_state = get_state(node)\n",
    "\n",
    "                    if not fallback_used:\n",
    "                        agent.replay_buffer.append((state, action, reward, next_state, False))  \n",
    "                    else:\n",
    "                        # if we used fallback, we need to add the fallback action to the replay buffer\n",
    "                        fallback_reward = get_reward(fallback_action, equivalent, num_llm_calls + fallback_llm_calls, alpha, beta)\n",
    "                        total_reward += reward\n",
    "                        agent.replay_buffer.append((state, fallback_action, fallback_reward, next_state, False))\n",
    "\n",
    "                    if action == 2 or action == 3:  # If the action was Child or ResampleTreeDecompositionThenSolve\n",
    "                        num_children = len(node.get(\"sons\", []))\n",
    "                        child_reward = reward / num_children  \n",
    "                        for child_state, child_action, _, next_child_state, done in child_experiences:\n",
    "                            agent.replay_buffer.append((child_state, child_action, child_reward, next_child_state, done))\n",
    "\n",
    "                    # update total number of llm calls in this episode\n",
    "                    total_num_llm_calls += num_llm_calls\n",
    "\n",
    "                    agent.train(batch_size)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "        rewards_list.append(total_reward)\n",
    "        accuracy_list.append(accuracy)\n",
    "        total_num_llm_calls_list.append(total_num_llm_calls)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_agent = copy.deepcopy(agent)\n",
    "\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Total LLM Calls in this episode: {total_num_llm_calls}\")\n",
    "        print(f\"Epsilon: {epsilon:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Store the rewards and accuracy for this configuration\n",
    "    accuracy_rewards_numcalls[config_name] = {\n",
    "        \"rewards\": rewards_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"num_calls\": total_num_llm_calls_list\n",
    "    }\n",
    "\n",
    "    # Save model after training with each configuration\n",
    "    model_path = f\"saved_models_with_resampling_trained_all/agent_{config_name}.pth\"\n",
    "    torch.save(agent.q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Save the best model overall\n",
    "    if best_agent is not None:\n",
    "        best_model_path = f\"saved_models_with_resampling_trained_all/best_agent_{config_name}.pth\"\n",
    "        torch.save(best_agent.q_network.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get rewards, accuracy and number of LLM calls for each configuration\n",
    "balanced_rewards = accuracy_rewards_numcalls[\"Balanced\"][\"rewards\"]\n",
    "balanced_accuracy = accuracy_rewards_numcalls[\"Balanced\"][\"accuracy\"]\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"]\n",
    "\n",
    "efficiency_rewards = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"rewards\"]\n",
    "efficiency_accuracy = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"accuracy\"]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"]\n",
    "\n",
    "high_accuracy_accuracy = accuracy_rewards_numcalls[\"High_Accuracy\"][\"accuracy\"]\n",
    "high_accuracy_rewards = accuracy_rewards_numcalls[\"High_Accuracy\"][\"rewards\"]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"]\n",
    "\n",
    "# min max scaling for rewards\n",
    "balanced_rewards = (balanced_rewards - np.min(balanced_rewards)) / (np.max(balanced_rewards) - np.min(balanced_rewards))\n",
    "efficiency_rewards = (efficiency_rewards - np.min(efficiency_rewards)) / (np.max(efficiency_rewards) - np.min(efficiency_rewards))\n",
    "high_accuracy_rewards = (high_accuracy_rewards - np.min(high_accuracy_rewards)) / (np.max(high_accuracy_rewards) - np.min(high_accuracy_rewards))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(balanced_rewards, label=\"Balanced\")\n",
    "plt.plot(efficiency_rewards, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_rewards, label=\"High Accuracy\")\n",
    "plt.title(\"Normalized Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Normalized Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(balanced_accuracy, label=\"Balanced\")\n",
    "plt.plot(efficiency_accuracy, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_accuracy, label=\"High Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(balanced_num_calls, label=\"Balanced\")\n",
    "plt.plot(efficiency_num_calls, label=\"Efficiency Focused\")\n",
    "plt.plot(high_accuracy_num_calls, label=\"High Accuracy\")\n",
    "plt.title(\"Number of LLM Calls\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Calls\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of calls for each configuration\n",
    "balanced_num_calls = accuracy_rewards_numcalls[\"Balanced\"][\"num_calls\"][-1]\n",
    "efficiency_num_calls = accuracy_rewards_numcalls[\"Efficiency_Focused\"][\"num_calls\"][-1]\n",
    "high_accuracy_num_calls = accuracy_rewards_numcalls[\"High_Accuracy\"][\"num_calls\"][-1]\n",
    "\n",
    "print(\"balanced_num_calls\", balanced_num_calls)\n",
    "print(\"efficiency_num_calls\", efficiency_num_calls)\n",
    "print(\"high_accuracy_num_calls\", high_accuracy_num_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it on test set data of hotpot qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent on the test data\n",
    "def evaluate_agent(agent, data, q2gold):\n",
    "    agent.q_network.eval()  # Set the model to evaluation mode\n",
    "    correct_answers = 0\n",
    "    total_parent_nodes = 0\n",
    "    results = []\n",
    "\n",
    "    for example in data:\n",
    "        for node in example:\n",
    "            if \"fa\" not in node:  # Only process root nodes (original questions)\n",
    "                gold_answer, dataset_used = q2gold[node[\"question_text\"]]\n",
    "                total_parent_nodes += 1\n",
    "                state = get_state(node)\n",
    "                action = agent.select_action(state, epsilon=0)\n",
    "                # print(\"action selected \", action)\n",
    "                done_resampling = False\n",
    "\n",
    "                # saving original node question because maybe after resampling the question is changed\n",
    "                original_node_question = node[\"question_text\"]\n",
    "                if action == 3: # ResampleTreeDecompositionThenSolve\n",
    "                    root_question = node[\"question_text\"]\n",
    "                    # print(\"example before resampling\", example)\n",
    "                    example = tree_resampling_pipeline.resample_tree(root_question)\n",
    "                    # print(\"example after resampling\", example)\n",
    "                    # get the parent node again because maybe the tree is changed in term of the order\n",
    "                    for node_ in example:\n",
    "                        if \"fa\" not in node_:\n",
    "                            node = node_\n",
    "                            break\n",
    "                    done_resampling = True\n",
    "                    action = 2 # Set action now to Child to execute child aggregation\n",
    "                chosen_answer = None\n",
    "                fallback_used = False\n",
    "                fallback_action = None\n",
    "\n",
    "                # Simulate the action\n",
    "                if action == 0:\n",
    "                    node['cb_answer'] = get_cb_answer(node[\"question_text\"].strip(), dataset_used)\n",
    "                    chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                    # chosen_answer = node.get(\"cb_answer\", [None])\n",
    "                elif action == 1:\n",
    "                    # since you are the parent, so topic entities are empty because no references\n",
    "                    node['ob_answer'] = get_singlehop_ob_answer(node[\"question_text\"].strip(), [], dataset_used)\n",
    "                    chosen_answer = node.get(\"ob_answer\", [None])\n",
    "                elif action == 2:\n",
    "                    # Call child aggregation function\n",
    "                    _, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                    \n",
    "                if done_resampling:\n",
    "                    # if the original action was resampling, then reset the action to 3\n",
    "                    # print(\"original action was resampling, so set action to 3\")\n",
    "                    action = 3 \n",
    "                \n",
    "                # Get Q-values for the current state deep copy\n",
    "                q_values = agent.q_network(state).clone()\n",
    "                while (chosen_answer is None or \"unknown\" in chosen_answer[0].lower().strip()):\n",
    "                    fallback_used = True    \n",
    "\n",
    "                    # Mask the Q-value of the originally chosen action\n",
    "                    q_values[action] = -float('inf')  # Ignore the originally chosen action\n",
    "\n",
    "                    # if all q_values are -inf, then break\n",
    "                    if torch.all(q_values == -float('inf')):\n",
    "                        print(\"All actions are invalid, skipping fallback\")\n",
    "                        break\n",
    "\n",
    "                    # Mask invalid actions if applicable\n",
    "                    # For example, if state[150] == 0 (has_children == 0), disable actions 2 (Child) and 3 (Resample)\n",
    "                    if state[150] == 0:\n",
    "                        q_values[2] = -float('inf')  # Child aggregation\n",
    "                        q_values[3] = -float('inf')  # ResampleTreeDecomposition\n",
    "\n",
    "                    # Select the next best action\n",
    "                    fallback_action = torch.argmax(q_values).item()\n",
    "                    \n",
    "                    # set q_values of the fallback action to -inf in order to not select it again\n",
    "                    q_values[fallback_action] = -float('inf')  # Ignore the originally chosen action\n",
    "\n",
    "                    # Execute the fallback action\n",
    "                    if fallback_action == 0:  # CB\n",
    "                        chosen_answer = get_cb_answer(node[\"question_text\"].strip(), dataset_used)\n",
    "                    elif fallback_action == 1:  # OB\n",
    "                        chosen_answer = get_singlehop_ob_answer(node[\"question_text\"].strip(), [], dataset_used)\n",
    "                    elif fallback_action == 2:  # Child\n",
    "                        # Call your tree decomposition/resampling logic if required\n",
    "                        _, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                    elif fallback_action == 3:  # Resample\n",
    "                        # Call your tree resampling logic if required\n",
    "                        example = tree_resampling_pipeline.resample_tree(node[\"question_text\"].strip())\n",
    "                        _, chosen_answer = child_aggregation(example, dataset_used)\n",
    "                    \n",
    "                    # print(\"Fallback used, \", \"original action:\", action , \"fallback action:\", fallback_action, \"chosen answer:\", chosen_answer)\n",
    "\n",
    "                # Compute reward\n",
    "                # gold_answer, _ = q2gold[original_node_question]\n",
    "                # if normalize_answer(chosen_answer[0]) == normalize_answer(gold_answer):\n",
    "                if chosen_answer and are_answers_equivalent_using_llm(gold_answer, chosen_answer[0]):\n",
    "                    correct_answers += 1\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"idx\": node[\"idx\"],\n",
    "                    \"question\": original_node_question,\n",
    "                    \"answer\": chosen_answer[0],\n",
    "                    \"gold\": gold_answer,\n",
    "                    \"tree-decomposition\": example,\n",
    "                    \"method\": action_space[action] if not fallback_used else action_space[fallback_action]\n",
    "                })\n",
    "\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return results, correct_answers, total_parent_nodes\n",
    "\n",
    "\n",
    "# List of trained models to evaluate\n",
    "configurations = [\n",
    "    {\"alpha\": 2.0, \"beta\": 0.05, \"name\": \"High_Accuracy\"},\n",
    "    {\"alpha\": 1.0, \"beta\": 0.1, \"name\": \"Balanced\"},\n",
    "    {\"alpha\": 0.5, \"beta\": 0.2, \"name\": \"Efficiency_Focused\"},\n",
    "]\n",
    "\n",
    "predictions_per_config = {}\n",
    "correct_answers_per_config = {}\n",
    "total_parent_nodes_per_config = {}\n",
    "accuracy_per_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    agent_model_path = f\"saved_models_with_resampling_trained_all/agent_{config_name}.pth\"\n",
    "    best_model_path = f\"saved_models_with_resampling_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model agent and best agent\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(agent_model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(best_model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_hotpotqa, q2gold_test_hotpotqa)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    agent_model_path = f\"saved_models_with_resampling_trained_all/agent_{config_name}.pth\"\n",
    "    best_model_path = f\"saved_models_with_resampling_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model agent and best agent\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(agent_model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(best_model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_2wiki, q2gold_test_2wiki)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    agent_model_path = f\"saved_models_with_resampling_trained_all/agent_{config_name}.pth\"\n",
    "    best_model_path = f\"saved_models_with_resampling_trained_all/best_agent_{config_name}.pth\"\n",
    "\n",
    "    # Load the trained model agent and best agent\n",
    "    agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    agent.q_network.load_state_dict(torch.load(agent_model_path))\n",
    "    agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    best_agent = DQNAgent(state_dim, action_dim, hidden_dim, lr, gamma)\n",
    "    best_agent.q_network.load_state_dict(torch.load(best_model_path))\n",
    "    best_agent.q_network.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\nEvaluating model: {config_name}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    rl_predictions, correct_answers, total_parent_nodes = evaluate_agent(best_agent, test_data_musique, q2gold_test_musique)\n",
    "    predictions_per_config[config_name] = rl_predictions\n",
    "    correct_answers_per_config[config_name] = correct_answers\n",
    "    total_parent_nodes_per_config[config_name] = total_parent_nodes\n",
    "\n",
    "    # Print final accuracy\n",
    "    accuracy = (correct_answers / total_parent_nodes) * 100 if total_parent_nodes > 0 else 0\n",
    "    accuracy_per_config[config_name] = accuracy\n",
    "    print(f\"Final Accuracy for {config_name}: {accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Guided-Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
